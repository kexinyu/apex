[33mcommit dde72915cd1aa4c9f620fa9a55d6fa70ce9d644e[m[33m ([m[1;36mHEAD -> [m[1;32mkyu/test_lamb_update[m[33m, [m[1;31morigin/kyu/test_lamb_update[m[33m)[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Aug 19 10:52:05 2020 -0700

    fix typo

[33mcommit 26c7f173b8aedf686e5ded3452e4871a131fc2f0[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Aug 19 10:29:32 2020 -0700

    less prints

[33mcommit 96a34966f0f710765527e128c0d3df1707219b58[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Aug 19 09:32:03 2020 -0700

    more printing

[33mcommit b42432a2a44c9fd7083a094700f1bc3e9f6413f6[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Aug 18 22:48:13 2020 -0700

    less printing

[33mcommit f8e98ac0986b4cb4bc65250df59cc85ee32c339b[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Aug 18 21:12:09 2020 -0700

    more prints

[33mcommit 9e1c488ecb54a6b13ceff6bec7f8d6a998e7f9af[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Aug 18 19:57:14 2020 -0700

    more prints

[33mcommit 119096d0b2a72589fca6a019d0e8b5b8573e22c7[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Aug 18 17:12:45 2020 -0700

    missing ;

[33mcommit 9c8b3497e7a9eeaf0c9d45632e11830aee1b9819[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Aug 18 17:00:57 2020 -0700

    print intermediates from kernel

[33mcommit 76b93cd7586d05ad807ef33b17963820d14f6d9c[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Aug 18 16:59:39 2020 -0700

    print intermediates from kernel

[33mcommit 1e8e895e480429dca24c39b5fe6a5e06df663f42[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Aug 18 16:14:50 2020 -0700

    fix print

[33mcommit 52e2bb703f9fe6299a3ecdc032d2d01073abccb4[m
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Tue Aug 18 15:08:40 2020 -0700

    fix prints

[33mcommit 67382ee04e4fc9a5dd91122008b87f03f68f794a[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Aug 18 13:47:20 2020 -0700

    more prints

[33mcommit 6ea0aa73abfc48a07764fd35e5c007ea349237d8[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Aug 18 11:45:08 2020 -0700

    fix print

[33mcommit 8afe38ba3bcd25534a7369ead928507595ae5131[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Aug 18 09:39:59 2020 -0700

    debugging

[33mcommit 56c98eab2de53cce62bda18004e31063b28496b7[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Aug 17 22:21:25 2020 -0700

    fix signature

[33mcommit 551a20351e085ad4d48eb7b25f4314bbf104c02e[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Aug 17 21:25:31 2020 -0700

    no pointer

[33mcommit da23509aeaf0f8e685851262bf8b653b903508d0[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Aug 17 20:00:07 2020 -0700

    not using pointer

[33mcommit 6334ad2ecd1102e923576c165247a2a10aa8f0f0[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Aug 17 17:52:52 2020 -0700

    more printings

[33mcommit 6aaeaa37fb78dde0c8bd74a52a33976edeae39f9[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Aug 17 17:50:17 2020 -0700

    more printing

[33mcommit 1764cab4a91e52028a76f920c6989ebf08472269[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Aug 17 17:21:38 2020 -0700

    more prints

[33mcommit 2c36fe7a9d98fdeaadf7adb09ab31202021c0e35[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Aug 17 14:33:54 2020 -0700

    print update, param norm, update norm

[33mcommit 6852f3a73c831a4867ba05135d9e194caa525c7e[m[33m ([m[1;31morigin/kyu/mp_grad_norm[m[33m, [m[1;31morigin/kyu/distopt_group_support[m[33m, [m[1;32mkyu/mp_grad_norm[m[33m, [m[1;32mkyu/distopt_group_support[m[33m)[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Aug 5 20:55:16 2020 -0700

    fix param broadcast

[33mcommit ddb090ca3b53e01337deecfa8a2c1bbefafeb695[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Aug 5 17:10:07 2020 -0700

    output broadcast shape match

[33mcommit 1c0999024483d596b17dbcab85ab4fb70b83f50a[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Aug 5 16:04:22 2020 -0700

    right way to subtract overap

[33mcommit 94d3703bc06d33e6caf436cb78294af105e1dc44[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Aug 5 15:01:58 2020 -0700

    corner case: no non-parallel grads

[33mcommit 96ec9b9fbcbf21c815445dad29230240653be92f[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Aug 5 14:02:28 2020 -0700

    more prints

[33mcommit 14dccd328afd5f518dc55d47a108a2073202c911[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Aug 4 17:14:35 2020 -0700

    debugging

[33mcommit fc8ff41ec728fee0a3993d030dc05e95d0285931[m
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Tue Aug 4 10:00:06 2020 -0700

    grad norm: handle model parallel parameters

[33mcommit cd687cceef6fcb5d7d7ebbb7fb18e74938363dc5[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Aug 3 11:59:44 2020 -0700

    disable broadcast

[33mcommit 0a1e9c38f3f9c2fd231ec24cbed537324802968d[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Aug 3 11:15:59 2020 -0700

    broadcast param from rank 0 of current process group rather than global rank 0

[33mcommit 853535539a9b3ac902fb7bea7186ca0b3b27d850[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu Jul 30 16:01:24 2020 -0700

    add option for gradient norm clipping

[33mcommit 3376f60399296af73a1ad747f94c2ee0b9d15d6c[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sun Jul 26 19:36:03 2020 -0700

    integer division for rank calculation

[33mcommit 049206cbeae6eaacd0530a6a13131635452f2736[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sun Jul 26 17:13:25 2020 -0700

    typo

[33mcommit 454d2c6af9098f5a42dfb4def405a260d275f8f2[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sun Jul 26 15:39:23 2020 -0700

    process group experiment

[33mcommit c925af953e2b86a202be011b8624db60b16d11ad[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sun Jul 26 09:08:21 2020 -0700

    revert

[33mcommit 9769857fc26bd65d0ef00661eab3b327aaefedc7[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sat Jul 25 20:51:34 2020 -0700

    remove collectives for testing

[33mcommit 0562513f4ca8ab2f30cff21ebc73cefa08545807[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sat Jul 25 14:00:55 2020 -0700

    debugging

[33mcommit 5f4680b6a666bc3db8d6be41982ad7db90500d95[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sat Jul 25 10:14:36 2020 -0700

    fix group init

[33mcommit a6b9f1154dc11f5285fb630d910369ab60d913b8[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sat Jul 25 08:54:41 2020 -0700

    fix new group init for all-reduce, reduce-scatter and all-gather

[33mcommit f8f9a2d621cfeb6d71582552e112e1a24e9bbf96[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Fri Jul 24 21:39:55 2020 -0700

    rank debugging

[33mcommit 4dd8f0bd7ae9f91a7a503a362f09cae664d0323e[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu Jul 23 13:22:56 2020 -0700

    attribute name typo

[33mcommit c90e7476f3b506ddc807e2c0d4328ed4c44e4d57[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu Jul 23 12:23:54 2020 -0700

    more debugging prints

[33mcommit dd5f3e4175e6191537b8ae1eb68e06842acf6e48[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu Jul 23 11:18:52 2020 -0700

    debugging process group

[33mcommit 4554213f92f31c595d4815e127bbf861e9e9cef4[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu Jul 23 10:19:11 2020 -0700

    temporarily disable all_reduce to WAR NCCL err

[33mcommit 48faffebac27aedd819d93378a4b8b653fc47aac[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sun Jul 19 18:59:38 2020 -0700

    use param group lr

[33mcommit afdba9e5993e80d11e74bc4b2954f57b2e1e1764[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sun Jul 19 17:40:44 2020 -0700

    move lr out of group properties

[33mcommit bb58a3643b6d55f209cd8a310d666148c15f8de2[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sat Jul 18 15:11:22 2020 -0700

    fix beta2

[33mcommit b2d32dda6ba152cd9ca08948664c1ffa610a4dba[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Fri Jul 17 12:38:44 2020 -0700

    fix naming

[33mcommit f471cada72e47607a3c1b347496f41dd49ad4cb1[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Fri Jul 17 11:21:23 2020 -0700

    define adam

[33mcommit 7ef9884cac50569e3c50ce806061cd9b4380903d[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Fri Jul 17 00:05:28 2020 -0700

    import distributed_adam_cuda

[33mcommit a8f64f98a2718639b43c0162913731831ec2c9fb[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu Jul 16 23:15:51 2020 -0700

    get beta1, beta2 from group['betas']

[33mcommit d336389742a932584ec9d87fe758a6e7e7508c87[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu Jul 16 15:07:01 2020 -0700

    first commit: add process group support for distopt

[33mcommit 44f3d3e5f6bd9c08142513bd3b5c792b0d5395d4[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jul 13 22:31:52 2020 -0700

    fix typos

[33mcommit 61513ffcc7de8b28ed33b8386a52a306b621cbe2[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jul 13 21:14:04 2020 -0700

    fix typos & missing definitions

[33mcommit df379d279186f734b8b760882ac2342e8103337f[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jul 13 21:07:13 2020 -0700

    should use int for bias_correction

[33mcommit cfe376be0027be494e94875d70a8f50dc68f0f22[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jul 13 21:02:32 2020 -0700

    should be comma

[33mcommit 171de99cd0bfb3eede57e95911b20345a244c52c[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jul 13 20:57:01 2020 -0700

    missing semicolons

[33mcommit dc5a0f3792e46fcbc97f8082c6bc2aa668434f0a[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jul 13 20:41:25 2020 -0700

    use AccumulateType for p, m, v

[33mcommit 662e990c6f384a0edd1f04d558972d6bf7ff1f78[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jul 13 20:16:15 2020 -0700

    missing include

[33mcommit 366621fae7e6a7d89a63dfd3d007a66239701397[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jul 13 20:02:51 2020 -0700

    missing semicolon for struct

[33mcommit ffe124d00a9189d6912b10fa8bd85b6d1e7044d6[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jul 13 19:56:36 2020 -0700

    missing semicolon

[33mcommit 4e294dc5cec254acb920ed96213bfed25d3050ab[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jul 13 19:52:25 2020 -0700

    rename files

[33mcommit 2feca5415172668e812391e5cb7bc1dd2c5921a1[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jul 13 19:27:40 2020 -0700

    first commit: distributed fused adam that supports multiple param groups

[33mcommit 2eb994728b5d60330efead118896e8dc1449dfe9[m
Merge: 932ef31 1ff54b8
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jul 13 17:24:00 2020 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 1ff54b8fed441c39dac181091b44fecdca31a403[m[33m ([m[1;31mupstream/master[m[33m)[m
Author: jjsjann123 <jiej@nvidia.com>
Date:   Mon Jul 6 14:47:30 2020 -0700

    [sync BN] (#792)
    
    * [sync BN]
    
    support non-uniform batch size across process group.
    
    TODO: test should be added once cleaned up.
    
    * updating unit tests
    
    * new unit tests for different inputs
    
    * cleaning

[33mcommit 932ef3121c76220b984e1960cc2055076007e012[m[33m ([m[1;31morigin/test_distributed[m[33m)[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Jul 1 13:49:20 2020 -0700

    add flag for DistributedAdam: step_support_amp_scaling

[33mcommit 43a6f9fe91c242170cbc5c8bf13f466eaccab2e4[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Tue Jun 30 17:09:32 2020 -0600

    Don't patch tensor ops that aren't present   (#899)
    
    * Only attempt to patch Tensor methods if defined
    
    * syntax
    
    Co-authored-by: Michael Carilli <mcarilli@nvidia.com>

[33mcommit 44532b30a4fad442f00635a0f4c8f241b06c2315[m
Merge: c3fad1a ad50ce9
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Jun 23 13:37:28 2020 -0700

    Merge pull request #892 from kexinyu/master
    
    add unit tests for FusedLAMB optimizer

[33mcommit ad50ce9a89bed49235d8eeb87ea1d6f8cc8d6036[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Jun 23 13:34:03 2020 -0700

    add test case for non-zero weight decay

[33mcommit cd3d6d12dcc011f4aac1e6606147227e5c79d4f7[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Jun 23 09:52:04 2020 -0700

    test nvlamb; hyperparams consistent with adam/adagrad tests

[33mcommit 9774ce0d3fa6033059027526d9e762421282a9ad[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Jun 22 17:17:27 2020 -0700

    add test for FusedLAMB

[33mcommit c3fad1ad120b23055f6630da0b029c8b626db78f[m
Merge: 02a3387 73ff00e
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Mon Jun 15 15:43:14 2020 -0500

    Merge pull request #885 from a-maci/2dmasking_sparsity
    
    2d masking and sparsity

[33mcommit 73ff00eaed78f19ca6c22d512be3aa1b4901b7bf[m
Author: Asit <asit.03in@gmail.com>
Date:   Mon Jun 15 13:41:04 2020 -0700

    Updating comment
    
    Minor edit

[33mcommit 8923046fef54b68b01991c767e60942faf588372[m
Author: Asit <asit.03in@gmail.com>
Date:   Mon Jun 15 13:22:45 2020 -0700

     adding comments for 2d pruning
    
    Importance and usage is 2d masking

[33mcommit ceca097fc54bce8deeffbb29ccbde98396ab5310[m
Author: Asit <asit.03in@gmail.com>
Date:   Mon Jun 15 13:18:16 2020 -0700

    editing comments

[33mcommit 02a33875970e1b555754dfc4ab85d05595d23764[m
Merge: 058addb 060bd5c
Author: schetlur <sharanyan.chetlur@gmail.com>
Date:   Thu Jun 11 16:36:36 2020 -0700

    Merge pull request #883 from NVIDIA/schetlur/stream_bug_fix
    
    Update softmax.h

[33mcommit 060bd5cd9212bf2e1a9ac754d1863b594f2ffd0a[m
Author: schetlur <sharanyan.chetlur@gmail.com>
Date:   Thu Jun 11 11:43:40 2020 -0700

    Update softmax.h
    
    Fixing NULL stream launch in dispatch_additive_masked_softmax

[33mcommit 058addbec0e7e084f1c93809928d545484388387[m
Merge: 3e474e8 1574c03
Author: Cliff Woolley <cliffwoolley@users.noreply.github.com>
Date:   Wed Jun 10 16:30:05 2020 -0700

    Merge pull request #880 from seryilmaz/seryilmaz/stream
    
    add streaming support for softmax kernels

[33mcommit 1574c03d42a73b5473622c9f7cc80902a829615b[m
Author: Sukru Eryilmaz <seryilmaz@computelab-dgx1v-32.nvidia.com>
Date:   Wed Jun 10 19:02:41 2020 -0400

    add streamid

[33mcommit 16869317ee7d31ae2376dfa4a9c528ae7f1ffdfa[m
Author: Sukru Eryilmaz <seryilmaz@computelab-dgx1v-32.nvidia.com>
Date:   Wed Jun 10 18:36:29 2020 -0400

    add streaming suppport for softmax kernels

[33mcommit 3e474e8567ad1585c3a03e56ae9c56367762773c[m
Merge: 097238f 7366698
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Jun 10 14:42:41 2020 -0500

    Merge pull request #878 from ksivaman/sparsity
    
    Sparsity

[33mcommit 736669895d09bd609062dfb67fa8f6ffd1019750[m
Author: Kirthi Sivamani <ksivamani@nvdl-smc-02.nvidia.com>
Date:   Wed Jun 10 12:34:09 2020 -0700

    fixed import error

[33mcommit b152a8dd8dec73f343f4dec0626f0545989e3c5b[m
Author: Kirthi Sivamani <ksivamani@nvdl-smc-02.nvidia.com>
Date:   Wed Jun 10 11:01:06 2020 -0700

    update README

[33mcommit 3028b18b373995f4fe477538e3d4bafac8238948[m
Author: Kirthi Sivamani <ksivamani@nvdl-smc-02.nvidia.com>
Date:   Wed Jun 10 10:54:39 2020 -0700

    added README for ASP API

[33mcommit 2a5b726a87b82df6e2de861cb480ae3b79c5d747[m
Author: Kirthi Sivamani <ksivamani@nvdl-smc-02.nvidia.com>
Date:   Wed Jun 10 10:53:32 2020 -0700

    asp files

[33mcommit 097238f8ea68132e54b40ed43c613e7231e8d24e[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Jun 1 10:09:37 2020 -0600

    Add Pyprof removal warnings that point to new repo (#862)
    
    Co-authored-by: Michael Carilli <mcarilli@nvidia.com>

[33mcommit 76026a3542f84adc53591f4982d50572a0a2ce4f[m
Merge: cdb0b3e 012135b
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Mon Jun 1 09:30:57 2020 -0500

    Merge pull request #867 from NVIDIA/fix_python_only_apex_distopt
    
    Remove distributed lamb from __init__.py

[33mcommit 012135b1c4160018f9de4c3073c1077f0046ac22[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Mon Jun 1 07:27:19 2020 -0700

    Remove distributed lamb from __init__.py

[33mcommit cdb0b3eeeac8ffa58d759f2d85d9ab9fe5803bf1[m
Merge: 5754fa7 55ae5fb
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 21:10:04 2020 -0500

    Merge pull request #863 from NVIDIA/distributed_lamb_optimizer
    
    Distributed lamb optimizer

[33mcommit 55ae5fbb639fd9bd494cde7ac33046f48c236239[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 19:08:28 2020 -0700

    Remove temporary file

[33mcommit b614a8551cd1231792f70a79040388c976bd1b8b[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 18:51:15 2020 -0700

    Remove verbose print-out

[33mcommit ba453a8e61700a2fb1df24851460fe2d337e3cf3[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sun May 31 00:57:29 2020 +0000

    WIP

[33mcommit 75f1e9d7d09e516c15b59f516909c074a20fd078[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sun May 31 00:52:27 2020 +0000

    Delayed init

[33mcommit 53cfd8c23dd24be4b25065636e6f375936b1179e[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 17:40:57 2020 -0700

    Delayed init

[33mcommit 4c54fd2bc2ab4abfd37c6ac926479969abf17e63[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 14:44:32 2020 -0700

    Make step global state variable

[33mcommit 5caf95ca69d1d1b8490e39ca75b61a3f5263aea2[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 14:42:09 2020 -0700

    Make step global state variable

[33mcommit 7741808be43a953395c6facfa9e3a7b504cd9fe4[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 14:30:14 2020 -0700

    Bug fix

[33mcommit 12458152b2ff4374242e7e9f923060963bfc7e42[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 14:28:05 2020 -0700

    Add more default values from regular lamb optimizer

[33mcommit 1e0aadd5884959333c81685e6d6e31f46d5dda00[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 14:22:50 2020 -0700

    Bug fix in update norm calculation

[33mcommit 0f64f6ad3303f49b331174dab8d3c71501347cd4[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 14:20:31 2020 -0700

    Bug fix in update norm calculation

[33mcommit 56650eb8eecf03af2fc80a21d15cc0cb6ea06191[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 14:16:01 2020 -0700

    Bug fix in update norm calculation

[33mcommit fb2d0f48ecb17a916bfd25a2098f8abdf792c95d[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 13:25:52 2020 -0700

    Bug fix

[33mcommit 3c02784b29df816dd9adaead8e49a5e113534a3e[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 13:18:41 2020 -0700

    Bug fix

[33mcommit 9773218c83279ffcc5db8c609fe8af21d954787e[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 13:15:31 2020 -0700

    Bug fix

[33mcommit 02fd73418a044a05b1bc3bdb8dba7418c71efb78[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 12:13:42 2020 -0700

    Add optional accumulation step

[33mcommit 9a09107cf638f383dde90974cfe96a013a38d615[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 11:51:27 2020 -0700

    Bug fix

[33mcommit 1ac28972ec4f45a034122d161cd109b90dcdfed0[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 11:39:53 2020 -0700

    Bug fix

[33mcommit ef43735898b6f242f96d2fa534af9b48d9bec804[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 11:08:48 2020 -0700

    Bug fix

[33mcommit e6925e6c70ce06e4c225f6d3e0af7585c6e82fa7[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 11:05:55 2020 -0700

    Bug fix

[33mcommit 8ed8eaacb15753588adbf2caf018bdb62a499c71[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 02:12:01 2020 -0700

    Use correct names for mt lamb cuda kernels

[33mcommit 45388d48574c291d00d8241e335f159187ecb78c[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 01:28:26 2020 -0700

    Make separate apex option for distributed lamb

[33mcommit 848a2844ad87c9be181d282c22a41922cda4568a[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 00:57:42 2020 -0700

    Remove unused forward def

[33mcommit 19892f1d7a80e3da133244056214e9809e433010[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Sat May 30 00:49:15 2020 -0700

    Distributed LAMB optimizer

[33mcommit 5754fa7a961b4b6dd7651436bd29dd5712bc134f[m
Author: Kevin Stephano <kevin.stephano@gmail.com>
Date:   Fri May 29 16:44:15 2020 -0700

    Fixes to Multihead Attention with LayerNorm and Dropout-Add (#860)

[33mcommit 6c2babf97adb795c55291e5e512ea5c3a50b0036[m
Author: Burc Eryilmaz <sberyilm@gmail.com>
Date:   Fri May 29 16:07:47 2020 -0700

    Fuses dropout and softmax in backward pass, add bias support to CPP MHA, add additive mask support, separate Q/K/V parameters (#854)
    
    Co-authored-by: Sukru Eryilmaz <seryilmaz@computelab-dgx1v-32.nvidia.com>

[33mcommit 36c9e904ebb2cfb42be350b36c01e587447ceb01[m
Merge: 87aca22 2be773d
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Fri May 29 15:31:20 2020 -0700

    Merge pull request #851 from kexinyu/master
    
    make FusedLAMB async

[33mcommit 87aca22aa3fe3a85729c0f0ca408fd9a68d2f17d[m
Author: Max V. Irgiznov <xeonvs@users.noreply.github.com>
Date:   Thu May 28 19:09:02 2020 +0200

    Fixed a typo (#856)

[33mcommit 5cb187f39bd9bd9a1a7d58b9ddd517684b962cea[m
Author: Kevin Stephano <kevin.stephano@gmail.com>
Date:   Tue May 26 23:48:50 2020 -0700

    Update Softmax in multihead attention to use the Current Cuda Stream instead of the Default Cuda Stream. (#843)
    
    * Adding C++ Multihead Attention implementation to contrib.
    
    * Add reference test that at least works for forward.
    
    * Remove CublasLt support from multihead attention.
    
    * Add new Python version of self attention.
    
    * Update python model of MHA with backward pass.
    
    * Fixed Output Linear connection in MHA.
    
    * Clean up compiles and add documentation to PySelfAttention.
    
    * Add Encdec Python version of multihead attention.  Cleanup files.
    
    * Tests for self and encdec multihead attention.
    
    * Add reference pytorch implementation of attention with norm and add.
    
    * Add cutlass branch definition.
    
    * Add cutlass download to compile.
    
    * Add norm/add tests.
    
    * Add biases to pytorch python versions.
    
    * Add tests and fix issues with python version of attention masking.
    
    * Create README.md
    
    * Update README.md
    
    * Update README.md
    
    * Update perf test parameters.
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md
    
    * Add files via upload
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md
    
    * Fix matmul1 output tensor size.  Fix tests that missed issue.
    
    * Allow for Z dimensions of 64K and greater on batched GEMMs.
    
    * remove redundant imports
    
    * general cleanup, remove deprecated or unused functions
    
    * Update Multihead Attention's softmax to use the Current Stream instead of the default stream.
    
    * Fix setup.py that got messed up in merge with upstream.
    
    * Update Multihead Attention strided batched gemms to use the current stream instead of the default.
    
    Co-authored-by: pbialecki <pbialecki@nvidia.com>

[33mcommit 2be773d3c57e5d8e9aad247a85a5f13fe64cab1e[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sat May 23 08:48:21 2020 -0700

    fix function signature

[33mcommit cf918ac18d9a15137697ec3144756a1d999a7272[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Fri May 22 14:46:59 2020 -0700

    more fixes on dtypes

[33mcommit 06a83ce742b09eae61f2d6acf44fb33aadc89038[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Fri May 22 13:01:30 2020 -0700

    use pointer

[33mcommit 4a1aa97e31ca87514e17c3cd3bbc03f4204579d0[m
Merge: 8abb690 3ccdfaa
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Fri May 22 11:35:24 2020 -0500

    Merge pull request #845 from NVIDIA/distopt_bug_fix
    
    Bug fix

[33mcommit 3ccdfaa39cd771ef6b0e767637f5d8191dc97270[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Fri May 22 09:28:37 2020 -0700

    Bug fix

[33mcommit 3a727a01f7b24595f4bf5a9333b878b04b796ef2[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu May 21 21:08:30 2020 -0700

    .data<...>()

[33mcommit 2c3f3d9a2b265ffa841d0ad749fb74a3ee009330[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu May 21 20:50:30 2020 -0700

    at::Tensor::data_ptr()

[33mcommit abc991da95f228a6125c34f1a314ba3c6e459b78[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu May 21 20:32:23 2020 -0700

    fix dtype

[33mcommit f54cc1c9d6cddbc8818d9a1f36b8bd5dc4f5e151[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu May 21 16:18:14 2020 -0700

    make fused LAMB async

[33mcommit 8abb6908e3b4f0c1c2cd516c45557e935d6357eb[m
Merge: 3bae8c8 bd6e66d
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue May 19 13:33:48 2020 -0700

    Merge pull request #819 from kexinyu/master
    
    Use global gradient clipping in FusedLAMB & add option for using NVLAMB

[33mcommit 3bae8c83494184673f01f3867fa051518e930895[m
Author: Andrew Tulloch <andrew@tullo.ch>
Date:   Thu May 14 00:10:20 2020 -0700

    Add FusedAdagrad (#822)

[33mcommit 9165b27fdf240f9bc08eac98b849a9d7c6308917[m
Author: Andrew Sears <asears@users.noreply.github.com>
Date:   Wed May 13 19:55:40 2020 -0400

    Fixes flake8 --select W605 test warnings (#829)
    
    Signed-off-by: asears <asears@users.noreply.github.com>

[33mcommit e1b7997a63babb73c3279ccecdc2bd3f61b8e462[m
Merge: cf50dc7 758826f
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Tue May 12 16:04:42 2020 -0500

    Merge pull request #753 from NVIDIA/revertable_fused_adam_with_mt_support
    
    Reversible fused adam with mt support

[33mcommit 758826fc0d749b0827194a8314ef2a1a572bfedd[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Mon May 11 17:24:14 2020 -0700

    Resolve possible race condition in stride_finite_check kernel

[33mcommit 0bfb830085a31c6ed49502c52403c8d0f1fd3eb9[m
Merge: 2619f1c cf50dc7
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Fri May 8 11:24:19 2020 -0700

    Merge

[33mcommit 2619f1cb668efe0d5453606fcc3b87785ba30c16[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu May 7 16:16:43 2020 -0700

    Resolve merge conflict

[33mcommit 91a5a87e7d88a8b4cc7bb4f6ef54dba027c36471[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed May 6 23:16:50 2020 -0700

    Slight improvements

[33mcommit 25c80afe5eb1248371a552e0eccb807966c45b5d[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed May 6 11:57:59 2020 -0700

    Re-introduce original non-reversible fused contrib adam cuda kernel

[33mcommit 9bb71066f20a46b26a435c48c39062c9c491515b[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed May 6 11:47:06 2020 -0700

    Revert regular contrib fused adam optimizer

[33mcommit 7e3536dd9acf0e5ec529bdaedb515cd3df6bafc6[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Tue May 5 22:27:42 2020 -0700

    Ultra-simple global all-reduce version of distributed optimizer

[33mcommit a60bbe63cfbb7a4a5c92898f0c39230c566f211b[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Mon May 4 17:41:48 2020 -0700

    Try out different partition scheme

[33mcommit 7da28fc39a8af80ea0d4aad573e65afc47d85195[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Mon May 4 10:42:54 2020 -0700

    Bug fix

[33mcommit bd6e66df95840c92e6dff3a8f38149bb3c5dbcd6[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sat May 2 11:06:45 2020 -0700

    initialize on device

[33mcommit 9033ad582ac3fd4458339e9ab71ece2438f28124[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sat May 2 10:10:20 2020 -0700

    initialize with tensor

[33mcommit f560bd0b83bf82d0a2e4806a3d52558570facaee[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sat May 2 08:40:52 2020 -0700

    save a sync when calculating global gradient norm

[33mcommit ac4ef2d613b92c3a7a4a64c199f25cbbf4d5d0f8[m
Merge: 85e4af7 cf50dc7
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Fri May 1 15:14:08 2020 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 85e4af76590cd71ee09491172f2cf70e08c9f7c9[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu Apr 30 21:45:15 2020 -0700

    make use_nvlamb a class attribute for FusedLAMB

[33mcommit cf50dc7c031295360c246a72ac03d22cdbb96354[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Thu Apr 30 19:12:32 2020 -0700

    Changes to make xentropysoftmax load/store vectorized when possible: (#725)
    
    * Changes to make xentropysoftmax load/store vectorized when possible:
    Increase default ILP so that each thread handle 16 Bytes data in one step
    Make thread load/store longest vector possible
    Make unroll case handle adjacent data instead of strided, so same order compare to vector case
    
    * Add shift for not aligned case. Remove less than 16 bytes aligned access

[33mcommit 3fd3e2c83a17ed2f88900933176dd4799a58a67c[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu Apr 30 17:55:40 2020 -0700

    add import

[33mcommit c8bcfff8143a36ea3724fd879a946b3daa79a70a[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu Apr 30 16:54:50 2020 -0700

    fix function signature for LAMBStage2Functor

[33mcommit 17ee854eaa737fd6769125d7bb1e3d4c42b1d707[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Thu Apr 30 13:21:22 2020 -0700

    enable wider load/store for multi_tensor_apply kernels (#763)
    
    * modify MTA axpby for wider load/store
    
    * Make scale/axpby/l2/adam/lamb multi_tensor uses wider load

[33mcommit 31aceeaa569631b4fa7fa60f3bd99b823e200a12[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Thu Apr 30 13:19:19 2020 -0700

    Improvements to apex.mlp (#804)
    
    * update fused bias relu backward kernel
    
    * adding support for not require first layer dgrad
    
    * fix bug: wrong layer in requires grad
    
    * add infrastructure for optional bias and activation, currently only support no bias and no relu
    
    * make bias and relu optional separately
    
    * add sigmoid activation option

[33mcommit aad9300bb04bed59f00b5a804980d2fab873260c[m
Author: Burc Eryilmaz <sberyilm@gmail.com>
Date:   Thu Apr 30 12:13:06 2020 -0700

    fix dropout scaling from p to 1/(1-p) (#816)
    
    Co-authored-by: Sukru Eryilmaz <seryilmaz@computelab-dgx1v-32.nvidia.com>

[33mcommit 9c82241dcd00cfba965033c3786ad35ff379e6aa[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Apr 30 09:43:26 2020 -0700

    Remove implicit memcpy of grad tensor in do_overlapped function

[33mcommit 5d1993cf53c7ba3b953b73919121f28b9dccd493[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Apr 29 18:23:18 2020 -0700

    Don't pad between consecutive parameters

[33mcommit e1a4deba632e8d2f48c22a3e3f3b513f90b9d465[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Apr 29 16:37:07 2020 -0700

    Bug fix

[33mcommit 415e26468ec82ebb9da48a7d1877a8559879c302[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Apr 29 14:46:02 2020 -0700

    Perf improvement (less CPU work)

[33mcommit 9d6d2e0197603e9651b71f386fb8bce1f973b53e[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Apr 29 12:22:29 2020 -0700

    Make L2 grad norm a CPU variable

[33mcommit bc81b1c1f81ab9fd5900abc4999744edecc4c5f2[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Apr 29 11:08:23 2020 -0700

    Bug fix

[33mcommit 44f547129d7e3e35eb1ad85c7265b1ef5c9592f7[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Tue Apr 28 17:26:19 2020 -0700

    Reduce CPU overhead, bigger step, all-gather

[33mcommit 5b3001192845b2f5e17f75853a6fe7eb799714f8[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Apr 28 14:39:39 2020 -0700

    LAMB: global grad clipping & more flexibility in adaptive lr

[33mcommit 1f2aa9156547377a023932a1512752c392d9bbdf[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Wed Apr 22 20:34:30 2020 -0700

    CUDAGenerator fix for #36026  (#801)
    
    * add CUDAGenerator guard
    
    * fix generator_flag
    
    * add guards for gen pointer/ref issue
    
    * change mutex_ to mutex()
    
    * add check_generator
    
    Co-authored-by: pbialecki <pbialecki@nvidia.com>

[33mcommit 71511fafdbdc06b04bb764a759e5b37aa0efe453[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Wed Apr 22 03:21:23 2020 -0700

    initial commit to add Multilayer Perceptron (MLP) extension (#790)

[33mcommit 2ec84ebdca59278eaf15e8ddf32476d9d6d8b904[m
Author: Vinicius Reis <vini@fb.com>
Date:   Wed Apr 22 06:19:38 2020 -0400

    Fix LARC with mixed precision (#793)
    
    The LARC optimizer wraps an underlying optimizer and then needs to be passed
    to amp.initialize for mixed precision. There were 3 different crashes happening
    in this situation, fix all of them and add a unit test.
    
    I don't know if the 'LARC' in sys.modules check ever worked. In my setup, the
    entry in sys.modules is 'apex.parallel.LARC'. Checking if the variable is
    defined seems more reliable though.

[33mcommit 55716d858878ec692d0f373a8dbb39a29f0952a5[m
Merge: 11faaca 04de0f7
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Apr 20 14:04:52 2020 -0700

    Merge pull request #761 from kexinyu/master
    
    add additional loop for lists of params in FP16_Optimizer's load_state_dict

[33mcommit f044805423348c0f6d317ba3a2b028f102c37d4e[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Mon Apr 20 11:45:23 2020 -0700

    Add alternate distributed optimizer implementation

[33mcommit 04de0f7a0b531d457d505e28cd5c3bc1e2cb2d95[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Apr 20 09:53:07 2020 -0700

    install option for contrib.optimizers.FusedLAMB

[33mcommit 4a01ff26237ec104b192e361335a237ac1c12c86[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Apr 16 15:57:51 2020 -0700

    Partial move towards syncfree optimizer

[33mcommit 2622d7f110e85837b52b7216e887ddc6c665907b[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Apr 16 12:57:11 2020 -0700

    Use glob_chunk to index streams and process groups

[33mcommit 854976324f47a26ca803166084cfc94c07f9cd54[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Apr 16 10:02:16 2020 -0700

    Bug fix

[33mcommit cef660ba5777114f0b495b9fed557008abf7ef20[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Apr 16 10:01:05 2020 -0700

    Bug fix

[33mcommit 6eca23897d423ef61220b0ee44fe5beab5bc29ba[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Apr 16 08:55:16 2020 -0700

    Pragmatic change, seems like WAR for NCCL crash

[33mcommit 2c744ee53aa50766499c3036b9f8489e60361352[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Apr 15 01:11:19 2020 -0700

    Bug(?) fix

[33mcommit 208c91e0f42061be2335e892a21eb4635a96a31d[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Tue Apr 14 22:26:43 2020 -0700

    internal pipelining more similar to micro-benchmarks

[33mcommit 11faaca7c8ff7a7ba6d55854a9ee2689784f7ca5[m
Author: Mannat Singh <mailmannatsingh@gmail.com>
Date:   Mon Apr 13 14:47:35 2020 -0400

    Return internal optimizer's param_groups from LARC (#767)

[33mcommit 7ba6a0388ebd16ce255d769b32c174474a8ff8f5[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Fri Apr 10 09:56:17 2020 -0700

    Add option to skip overflow check in step() method

[33mcommit c7b34549ece0000b32c5c958ef7dee33aea61aec[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Apr 9 22:26:14 2020 -0700

    Add no-flattening e5m2-allgather option

[33mcommit cd206434dc99ac7785bb8d5224c532984a00de9d[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Apr 9 13:49:39 2020 -0700

    Add e5m2 allgather option

[33mcommit aa90d31ff20bf99418c9861ad799b8ca6e879ff4[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Apr 8 10:15:15 2020 -0700

    Add internal pipelining option

[33mcommit be4c41c2018784a0ad672a02fe5020b44cf8b901[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Tue Apr 7 15:48:38 2020 -0700

    Bug fix

[33mcommit f3a960f80244cf9e80558ab30f7f7e8cbf03c0a0[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sun Apr 5 10:07:22 2020 -0700

    fix typo

[33mcommit d38e6fe44d8398847c31597a81b40d5d9db4b925[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Sun Apr 5 09:27:35 2020 -0700

    .item()

[33mcommit a0bf956a562786945d0d06152f77072da5fc39a5[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Fri Apr 3 16:59:32 2020 -0700

    more debugging

[33mcommit feb93a2a6eb760f7dd11caec50f20c3275924562[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu Apr 2 21:59:04 2020 -0700

    check empty lists

[33mcommit 8e5699e46adf0720358ffaa0dbc80227d53b42a4[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Thu Apr 2 17:54:36 2020 -0700

    more debugging

[33mcommit 9b96c824f331b61160b05930aa4cbf98ef0b5b03[m
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Thu Apr 2 17:03:35 2020 -0700

    seg fault debugging

[33mcommit db8fb9760e42a32db8c0e605cb9fd94058a4dbc8[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Apr 1 22:35:27 2020 -0700

    Add back support for multi tensor scale flattening

[33mcommit 921868638cf5c6641e430654e2da3878af3eeef2[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Apr 1 22:25:27 2020 -0700

    import amp_C.multi_tensor_l2norm

[33mcommit 3f717d956716717563d0475218ed2958b6a8db2c[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Apr 1 21:45:55 2020 -0700

    Bug fix in internal pipelining

[33mcommit 17160f34b9da0021fdc8f44aca9619737afedac0[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Apr 1 16:49:30 2020 -0700

    Bug fix

[33mcommit e5c8e3c903b0ff05e7f2c2a9b672b8b585bafdc0[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Apr 1 01:01:50 2020 -0700

    Add separate dwu_num_chunks argument

[33mcommit f2c9aa333b8336cf8e4eb6fe8fc21879b38ed2dc[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Apr 1 00:24:50 2020 -0700

    Add support for 4 all-reduce IB communicators

[33mcommit 5c1cf0201f25d677102e4438400bb3c5f5395d58[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Tue Mar 31 22:04:01 2020 -0700

    Move partial_step out of complete reductions:

[33mcommit 96b017a8b40f137abb971c4555d61b2fcbb87648[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Mar 31 21:53:43 2020 -0700

    add printing to test

[33mcommit 90729bc8d68f8c63624d16601fd89dcce58d1651[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Tue Mar 31 21:38:33 2020 -0700

    fix parameter type

[33mcommit 32d2c4e2eb29e0c17f3350b044d03385ef4573fc[m
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Tue Mar 31 09:33:03 2020 -0700

    clip gradients globally, rather than per group

[33mcommit ca00adace51ae4d82d2ff5cfc1ef1c9174ff6aaa[m
Author: Jeff Bowles <jbowles@riskybacon.com>
Date:   Tue Mar 31 01:53:02 2020 -0700

    Add support for bool datatype (#601) (#603)

[33mcommit 8fac3a7205188d2c95ac5cef74be3b8e05e372bd[m
Author: msbaines <35972327+msbaines@users.noreply.github.com>
Date:   Tue Mar 24 18:32:37 2020 -0700

    Fix contrib fused_adam to work correctly with multi-GPU (#752)
    
    The cuda kernel used by fused-adam was using the default stream
    on the default device. The kernel needs use the same device as
    the parameter tensor.
    
    Fixed by using context manager to set correct default device. For
    the use_mt case, raised an error. Alternatively, the use_mt
    case could launch one kernel per cuda device.
    
    The non-contrib version will also need to be fixed.
    
    Co-authored-by: Mandeep Singh Baines <msb@fb.com>

[33mcommit 8405d43634e007661c9440df09b39c55789d5e90[m
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Mon Mar 23 16:22:37 2020 -0700

    revert to gradient pre-normalization

[33mcommit a3ffb8a7c05a7e127b2120300186de697148c051[m
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Mon Mar 23 14:28:07 2020 -0700

    add l2norm source for FusedLAMB

[33mcommit 3f4fb81fa76eb6ee089260b6569711a41bface2f[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Mon Mar 23 11:12:47 2020 -0700

    Revert "Avoid 1 copy for double buffering scheme"
    
    This reverts commit 029cd5e1fe0bb126d017e04ed048d6fb4edf0050.
    
    .

[33mcommit 04927b3a0b2dbdba1d05cd651b91461958cb5fc8[m
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Sat Mar 21 11:36:07 2020 -0700

    fix typo

[33mcommit d8a78acb18235855e84cf279c06f30d4b79bef03[m
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Sat Mar 21 09:59:34 2020 -0700

    import name fix

[33mcommit 33f21d685bd9204d8453a2c138d19f9cac8b9a22[m
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Fri Mar 20 16:02:29 2020 -0700

    add FusedLamb in __init__

[33mcommit 604d423b0f11c555231531f9ac4dcb1470f8bb54[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Fri Mar 20 15:15:23 2020 -0700

    Broadcast parameters from rank 0 just to be safe

[33mcommit b4c32010d90a8460602bab74c197be3983d21dc8[m
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Fri Mar 20 15:13:13 2020 -0700

    extension name fix

[33mcommit bc98827b2960dfc8b93c5825141db90c238427cc[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Fri Mar 20 08:21:48 2020 -0700

    Bug fix

[33mcommit b222ed2baacca66e03209b53f98abe93a57b7dd5[m
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Thu Mar 19 17:43:10 2020 -0700

    apex.contrib.optimizers.FuseLamb first commit

[33mcommit 90795025c2787dc6a99bcaad3578d2114d110400[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 19 16:58:31 2020 -0700

    Bug fixes

[33mcommit 174abea72367c43c4a5cae3a92604bf4944b94a2[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 19 15:44:34 2020 -0700

    Bug fixes

[33mcommit dde1374145f153ccfb5d6d98dd5404fd543d17f6[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 19 13:33:15 2020 -0700

    Bug fix

[33mcommit 029cd5e1fe0bb126d017e04ed048d6fb4edf0050[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 19 09:10:19 2020 -0700

    Avoid 1 copy for double buffering scheme

[33mcommit b85ff391c0aaada91db8093a39e9688bf9bc954e[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Mar 18 18:57:38 2020 -0700

    Add option to revert step through double buffering

[33mcommit ffed6e803e522ebe6112b74faa94270bf9d343ef[m
Merge: eca3a2c 6b2ef78
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Mar 18 10:38:00 2020 -0700

    Merge branch 'revertable_fused_adam_with_mt_support' of https://github.com/NVIDIA/apex into revertable_fused_adam_with_mt_support
    
    .

[33mcommit eca3a2c458cf6a7cb5084a6cfa308dc861619742[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Mar 18 10:36:52 2020 -0700

    Change inplace to no_copy

[33mcommit 35e86d3ded5f8eb84e423df357ddf63ec4d62900[m
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Tue Mar 17 15:06:26 2020 -0700

    add additional loop for lists of params when loading state_dict in apex.contrib.optimizers.FP16_Optimizer

[33mcommit 93f91cdeb50347e1acfa88c38eb2dfd1f2621948[m
Merge: 33082d2 80b90b9
Author: Kexin Yu <kexiny@nvidia.com>
Date:   Tue Mar 17 15:01:36 2020 -0700

    Merge remote-tracking branch 'upstream/master'

[33mcommit 6b2ef78707ec8131517bf32cf8507cd4220bff07[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Mon Mar 16 21:08:16 2020 -0500

    Remove dead code

[33mcommit d662f9ca473d3dbec51e64c134b5b6cea79afc34[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Mon Mar 16 19:51:27 2020 -0500

    Rename inplace to no_copy to make effect clearer

[33mcommit 9f6c0da5e00c5aa0ba225697de3745bd327debc8[m
Merge: f91bc66 825cf27
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Fri Mar 13 11:56:56 2020 -0700

    Merge branch 'revertable_fused_adam_with_mt_support' of https://github.com/NVIDIA/apex into revertable_fused_adam_with_mt_support
    
    Rebasing

[33mcommit f91bc66b3dd95ba9d19de7fd08238fcc521a699b[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 18:59:59 2020 -0700

    Bug fix

[33mcommit a3743069206150bad64c453b87171c6f2add1b23[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 16:24:08 2020 -0700

    Bug fix

[33mcommit eb8384b51658d49b411f874682aec627aaefc121[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 15:49:33 2020 -0700

    Modify fused_adam to take advantage of undo feature

[33mcommit f1e565f5f4636e406e464f3ff7773f334ac9447f[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 15:47:01 2020 -0700

    Modify fused_adam to take advantage of undo feature

[33mcommit c659e5641fe8134d0e30a5233cfcc9362f391289[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 14:34:19 2020 -0700

    Add backwards compatible support for no inplace NCCL op

[33mcommit 68715149dd73197b9c906c10866063a256ba2b4a[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 12:39:32 2020 -0700

    Bug fix

[33mcommit 22c2be04cb38a410de8561347ae36acdf910698f[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 12:35:28 2020 -0700

    Bug fix

[33mcommit 9c7426952bc4824f6774e8be32d5c29b91ad19bb[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 09:16:30 2020 -0700

    Add distributed optimizer

[33mcommit 8759ce0ccba5945a94219d3ad01d779b65ca81d8[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 08:27:09 2020 -0700

    Bug fix

[33mcommit 79b2cc28efe4e432d025d36bc6067170ee875f97[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 07:33:40 2020 -0700

    Bug fix

[33mcommit bd6b1ebc8c9120c0309603d62c13b48b83488c3f[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 06:30:41 2020 -0700

    Bug fix

[33mcommit 1a994e3765088ee6382a70abf2874a4ba8201954[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Mar 11 17:12:19 2020 -0700

    First commit

[33mcommit 825cf27c8db5dfe8d309973f0c5aa8b34db61f94[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 18:59:59 2020 -0700

    Bug fix

[33mcommit 841e5ee128f29861d0dd66f781eade71e575f4fa[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 16:24:08 2020 -0700

    Bug fix

[33mcommit 1d4a95d46f90b349406df62c4f5377edeec36687[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 15:49:33 2020 -0700

    Modify fused_adam to take advantage of undo feature

[33mcommit d48218a0e9a14b57a73270cc11237e774ce30a99[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 15:47:01 2020 -0700

    Modify fused_adam to take advantage of undo feature

[33mcommit c73723205ea0433ca0f550a06fc0bbb32002776d[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 14:34:19 2020 -0700

    Add backwards compatible support for no inplace NCCL op

[33mcommit 400cf6284659a97f88e12a62caa74d0ee4502b7b[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 12:39:32 2020 -0700

    Bug fix

[33mcommit 9675775237e44be1ff008b62a4c2a4be090b4a6e[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 12:35:28 2020 -0700

    Bug fix

[33mcommit 1210d8fea2a4e24aafe565147ddc4de6002ce7e7[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 09:16:30 2020 -0700

    Add distributed optimizer

[33mcommit cfc4229eac3dbe638b15e1848a055c05f7784399[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 08:27:09 2020 -0700

    Bug fix

[33mcommit d2214aa469b90ba564cfa1dd2858f525a3f62f5b[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 07:33:40 2020 -0700

    Bug fix

[33mcommit 86ad07f7b8b11b24a5018fd4283860c39cb7f25b[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Mar 12 06:30:41 2020 -0700

    Bug fix

[33mcommit 8cc99c29b9ee9e752a2505e1f704481214a82ad9[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Mar 11 17:12:19 2020 -0700

    First commit

[33mcommit 80b90b9d43e343c7d4df7f6081a4ceaee2d3c348[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Wed Mar 11 16:15:52 2020 -0700

    Fix deprecated calls in multihead_attn and ninja build failure (#746)
    
    * disable ninja for multihead_attn
    
    * fix getCurrentStream in multihead_attn
    
    Co-authored-by: pbialecki <pbialecki@nvidia.com>

[33mcommit 20d00ab15c3883fcfc2396fad6084d8c243da99e[m
Author: Tomasz Grel <tomasz.grel@gmail.com>
Date:   Wed Mar 11 19:54:23 2020 +0100

    Do not unscale the gradients if loss scale equal to 1 (#748)
    
    * Do not unscale the gradients if loss scale equal to 1
    
    * Disable unscaling loss scale == 1 only for static scaling

[33mcommit 5633f6dbf7952026264e3aba42413f06752b0515[m
Author: pbialecki <pbialecki@nvidia.com>
Date:   Sun Mar 1 19:33:32 2020 -0800

    Revert "remove gencode from multihead_attn build (#731)"
    
    This reverts commit 92b3b9a971048c1db82a642ddc09e0f62b235f7c.

[33mcommit de6378f5dae8fcf2879a4be8ecea8bbcb9e59d53[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Feb 26 17:17:03 2020 -0800

    NHWC support for multi tensor apply (#732)
    
    * NHWC support for multi tensor apply
    
    * compilation fix for version<=1.4

[33mcommit 92b3b9a971048c1db82a642ddc09e0f62b235f7c[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Tue Feb 25 13:09:22 2020 -0800

    remove gencode from multihead_attn build (#731)

[33mcommit 5f6b9b0ec9ca0e1545b14ac3b143478a54c0017b[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Mon Feb 24 23:13:22 2020 -0800

    remove duplicated multihead_attn install (#729)

[33mcommit 93cabd5df060b3783228dd75941ad359be5d391d[m
Author: Saransh Karira <saransh661@gmail.com>
Date:   Tue Feb 25 07:55:33 2020 +0530

    Adding 'ctc_loss' to the list of FP32 funcs (#722)

[33mcommit 1733946a819b1f0a756aaee75a1e4d71addf08d9[m
Author: Kevin Stephano <kevin.stephano@gmail.com>
Date:   Mon Feb 24 10:57:11 2020 -0800

    Change to Multihead Attention to allow Batched GEMMs larger than 64K. (#728)
    
    * Adding C++ Multihead Attention implementation to contrib.
    
    * Add reference test that at least works for forward.
    
    * Remove CublasLt support from multihead attention.
    
    * Add new Python version of self attention.
    
    * Update python model of MHA with backward pass.
    
    * Fixed Output Linear connection in MHA.
    
    * Clean up compiles and add documentation to PySelfAttention.
    
    * Add Encdec Python version of multihead attention.  Cleanup files.
    
    * Tests for self and encdec multihead attention.
    
    * Add reference pytorch implementation of attention with norm and add.
    
    * Add cutlass branch definition.
    
    * Add cutlass download to compile.
    
    * Add norm/add tests.
    
    * Add biases to pytorch python versions.
    
    * Add tests and fix issues with python version of attention masking.
    
    * Create README.md
    
    * Update README.md
    
    * Update README.md
    
    * Update perf test parameters.
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md
    
    * Add files via upload
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md
    
    * Fix matmul1 output tensor size.  Fix tests that missed issue.
    
    * Allow for Z dimensions of 64K and greater on batched GEMMs.
    
    * remove redundant imports
    
    * general cleanup, remove deprecated or unused functions

[33mcommit 50338df6280fd47832039725ec5bdcc202591222[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Fri Feb 14 16:54:17 2020 -0800

    change include_dirs to abs path (#719)

[33mcommit 5b71d3695bf39efcdcda9dff5be2f70314b8f091[m
Author: Ayla Khan <1399377+a-y-khan@users.noreply.github.com>
Date:   Mon Feb 10 14:20:39 2020 -0700

    Fix opt_level command line arg in instructions. (#713)
    
    Actual flag is --opt_level and copy pasting the example results in an unrecognized arguments error.

[33mcommit 3f94528e9d25938e72dcdeea3d6217efce6c6607[m
Author: Kevin Stephano <kevin.stephano@gmail.com>
Date:   Thu Feb 6 12:02:12 2020 -0800

    Add Fast Multihead Attention to APEX Contrib (#697)
    
    * Adding C++ Multihead Attention implementation to contrib.
    
    * Add reference test that at least works for forward.
    
    * Remove CublasLt support from multihead attention.
    
    * Add new Python version of self attention.
    
    * Update python model of MHA with backward pass.
    
    * Fixed Output Linear connection in MHA.
    
    * Clean up compiles and add documentation to PySelfAttention.
    
    * Add Encdec Python version of multihead attention.  Cleanup files.
    
    * Tests for self and encdec multihead attention.
    
    * Add reference pytorch implementation of attention with norm and add.
    
    * Add cutlass branch definition.
    
    * Add cutlass download to compile.
    
    * Add norm/add tests.
    
    * Add biases to pytorch python versions.
    
    * Add tests and fix issues with python version of attention masking.
    
    * Create README.md
    
    * Update README.md
    
    * Update README.md
    
    * Update perf test parameters.
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md
    
    * Add files via upload
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md
    
    * Fix matmul1 output tensor size.  Fix tests that missed issue.

[33mcommit 494f8ab3fc1b0b26949a3bcbb2bcac78008d48c1[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Feb 5 10:46:48 2020 -0800

    Fix attribute name mismatch in state_dict() and load_state_dict() (#704)
    
    * updated apex.contrib.optimizers.FP16_Optimizer and FusedSGD
    
    * fix attribute name mismatch in state_dict() and load_state_dict()

[33mcommit 33082d2b45d44222b5a365fc74b8b5e266b5cbad[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Feb 5 10:17:10 2020 -0800

    fix attribute name mismatch in state_dict() and load_state_dict()

[33mcommit 858d7899a101d74f5f1abc9d9c38802d7d8a85e7[m
Merge: 8d2647f 2ca894d
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Feb 5 10:14:47 2020 -0800

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 2ca894da7be755711cbbdf56c74bb7904bfd8417[m
Author: Vitaly Fedyunin <vitalyf@fb.com>
Date:   Mon Jan 27 16:46:24 2020 -0500

    Channels last support (#668)

[33mcommit b66ffc1d952d0b20d6706ada783ae5b23e4ee734[m
Author: jjsjann123 <jiej@nvidia.com>
Date:   Mon Jan 20 20:04:01 2020 -0800

    removing build target sm_70 from bnp (#683)

[33mcommit b5a7c5f972bc644804408dd96d7cb515df3af07b[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Wed Jan 8 10:18:18 2020 -0800

    add WAR for pip>=19.3.1 (#652)
    
    * add WAR for pip>=19.3.1
    
    * remove pipmain, use extras_require instead

[33mcommit 0ce8ad3ee0cc21bffe2d799bad2cd2801e96ac29[m
Author: bonlime <bonlimezak@gmail.com>
Date:   Wed Dec 18 21:30:53 2019 +0300

    fix beta2 => beta1 (#661)

[33mcommit c19ee275395211a844d275957585f2d366a16625[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Wed Dec 18 10:28:07 2019 -0800

    updated apex.contrib.optimizers.FP16_Optimizer and FusedSGD (#657)

[33mcommit 8d2647f8d743dcdc1afb09d1eec98676e87777cd[m
Author: Kexin Yu <kexinznzn@gmail.com>
Date:   Mon Dec 16 17:04:32 2019 -0800

    updated apex.contrib.optimizers.FP16_Optimizer and FusedSGD

[33mcommit 4ad9b3bd4fab0c8f0e7811b369f59c234ee72ac7[m
Author: Neil Tenenholtz <ntenenz@users.noreply.github.com>
Date:   Wed Dec 4 22:13:30 2019 -0500

    Fixing typo in PyProf README (#637)

[33mcommit f37fdf07367a71521bd14fec66153e0996ad128c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Dec 3 10:02:14 2019 -0800

    Don't check if distributed is initialized on Windows

[33mcommit 82dac9c9419035110d1ccc49b2608681337903ed[m
Author: Roshan Rao <roshan_rao@berkeley.edu>
Date:   Fri Nov 22 09:20:03 2019 -0800

    update _amp_state to check distributed on maybe_print (#620)

[33mcommit 37cdaf4ad57ab4e7dd9ef13dbed7b29aa939d061[m
Author: jjsjann123 <jiej@nvidia.com>
Date:   Wed Nov 6 15:53:25 2019 -0800

    fixing batchnorm 1d input (#590)

[33mcommit 606c3dcccd6ca70f4b506714d38a193e0845ee7f[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Oct 30 09:49:14 2019 -0700

    Update README.md

[33mcommit 5b29cc13999a64e231cfc6cbe4ee6f35a57d4451[m
Author: Bram Vanroy <Bram.Vanroy@UGent.be>
Date:   Wed Oct 23 22:35:29 2019 +0200

    add gelu activation to fp32 list (#564)

[33mcommit 95d6c007ec9cca4231eefe0e5f6f4d0e52ea1517[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Oct 22 10:37:42 2019 -0700

    Making the encouragement to use O1 a bit stronger...

[33mcommit 4b913261a78ad881b482672d35a5265c3b812dd2[m
Author: Aron Hoffmann <aron.m.hoff@gmail.com>
Date:   Sun Oct 20 00:13:00 2019 +0100

    Made the patched optimizer step function a full method, not simply a function stored as an instance member (#553)

[33mcommit 088985936518be7e25795a30d8ab33affa9db6ed[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Oct 10 07:19:19 2019 -0700

    Adding presentation link to sphinx landing page

[33mcommit ec93c75bfda9dbc37dc11f56a786322289a8f026[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Oct 10 07:08:30 2019 -0700

    Adding links to references.  (maybe make this a subrepo?)

[33mcommit fab319f16f46c6815d82f54684e70736705b7d12[m
Author: Bram Vanroy <Bram.Vanroy@UGent.be>
Date:   Wed Oct 9 22:28:00 2019 +0200

    allow for non-distributed envs (Windows) (#531)

[33mcommit 753c427a6b694a1ff680ff8b22720e35977818bf[m
Author: Marek Kolodziej <mkolod@gmail.com>
Date:   Wed Oct 9 13:23:26 2019 -0700

    Fixed tensor core lookup for Turing (#534)

[33mcommit e87b5799e697c5ee45119e669d37bb06f6b975ac[m
Author: Jan Schlüter <github@jan-schlueter.de>
Date:   Tue Oct 8 02:10:54 2019 +0200

    Include loss scaling in README code example (#523)

[33mcommit 1904e48dba53402b54e148bf0bd517b72e9cabef[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Fri Oct 4 11:21:36 2019 -0700

    move previous fused_adam and fp16_optimizer to contrib (#517)
    
    * move previous fused_adam and fp16_optimizer to contrib
    
    * make build contrib.fused_adam optional
    
    * change build option name
    
    * remove unnecessary try import

[33mcommit 0b74bfd92ba0846ca29b9bd2c6dc18dd3a5d9b20[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Thu Oct 3 20:01:00 2019 +0200

    Disable tests for mixed opt_levels, add bitwise accurate test of parameters (#520)
    
    * increase atol for Half-Float comparison to 1.5e-4
    
    * disable tests for different opt_levels
    
    * reset atol
    
    * add bitwise accurate comparison

[33mcommit 03421e87c0aa212384abddd0a1aef44dafa7e8c3[m
Author: Timothee Cour <timothee.cour2@gmail.com>
Date:   Tue Oct 1 22:41:15 2019 -0700

    fix https://github.com/facebookresearch/maskrcnn-benchmark/issues/802 (#516)

[33mcommit 3ae89c754d945e407a6674aa2006d5a0e35d540e[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Sep 12 18:00:37 2019 -0700

    Seems to work locally (#490)

[33mcommit e6cb749b52f44de76cac564b20e3a66b6a837424[m
Author: Youngjin Kim <youngjin.kim@vision.snu.ac.kr>
Date:   Fri Sep 13 02:54:35 2019 +0900

    Fixed error in convert_syncbn_model function (#380)

[33mcommit ad98cc5f3fa2a17d8cea625e060013e9f6301ba8[m
Author: jjsjann123 <jiej@nvidia.com>
Date:   Wed Sep 11 15:40:28 2019 -0700

    removing nvtx range used for debugging (#485)

[33mcommit 325f5a0bec542701edba1628ad34f3b2ea47c556[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Sep 5 22:33:27 2019 -0700

    Fix for #456 (#477)
    
    * Pushing for build tests
    
    * Contrib files
    
    * Removing deprecated checks

[33mcommit 1bf0d8d4ba50b3ce06456f0757f15b73bbb65250[m
Author: Tony-Y <11532812+Tony-Y@users.noreply.github.com>
Date:   Fri Sep 6 12:14:40 2019 +0900

    LARC needs no Variable (#461)
    
    Remove torch.autograd.Variable

[33mcommit 7fa74925e74b6ae186293fbd26a411a09ba4d775[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Tue Sep 3 12:41:02 2019 -0700

    Fix issues in fused_dam (#469)
    
    * move import of amp_C to __init__()
    
    * make fp16/32 separate lists to support mixed param types, disable double test
    
    * make zero_grad consistent between adam/novograd/lamb

[33mcommit 35a8578986fc74283881d8413a23853cef759e0d[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Tue Sep 3 20:15:12 2019 +0200

    remove deprecated backaned.FunctionBackend calls (#466)

[33mcommit 53eae1986320d016ee7b347d78839dd5e96e7e93[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Thu Aug 29 17:24:36 2019 -0700

    [novograd] move exp_avg_sq to param device in load_state_dict (#459)

[33mcommit dec4fdd60039ee34dae41e4bc52f1fc6deb9ddeb[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Tue Aug 27 23:42:14 2019 +0200

    Enable Checkpointing (#420)
    
    * add state_dict, load_state_dict
    
    * add test_restoring, test_loss_scale_decrease
    
    * disable amp outputs for checkpoint tests
    
    * add test for amp.state_dict, cleanup
    
    * add state_dict patch, add test
    
    * fixed testing, cleanup
    
    * add readme for checkpointing
    
    * add docs to source/amp
    
    * add review changes to doc

[33mcommit 30ed793ef3c27b672c1a8509108ef4d457b0ae1c[m
Merge: b9e5d37 453eefa
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Aug 27 14:16:14 2019 -0700

    Deleting test_fp16_optimizer.py

[33mcommit b9e5d37dbdbfbf5596acce3b915acf4b1a93a789[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Aug 27 11:00:49 2019 -0700

    Docstring updates

[33mcommit 17e8a552499eb017c628e8ec3315f5c08ed1227e[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Aug 27 10:52:09 2019 -0700

    Docstring updates

[33mcommit ea7c2098d1efa0d8af3eb07785dfd6fe28e674ab[m
Merge: 427e82c 78c38db
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Aug 27 10:06:12 2019 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 427e82cd6b42c67cd4ba79a068e261743b0274c0[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Aug 27 09:59:32 2019 -0700

    Updating docstrings for fused optimizers

[33mcommit 15648029e9d44dccba981e4f07846b3acd799393[m
Merge: 880ab92 b9f0995
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Aug 26 17:02:40 2019 -0700

    Merge branch 'FDecaYed-deyuf/fused_optimizer_v2'

[33mcommit 78c38db467c378759b4278a4ca8547763da5c91d[m
Author: Du Xingjian <diggerdu@users.noreply.github.com>
Date:   Mon Aug 26 07:15:50 2019 +0800

    skip instancenorm in convert_syncbn_model (#438)

[33mcommit b9f0995b39d82608c3c264faf843fc2697b67512[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Tue Aug 20 16:04:02 2019 -0700

    add back lamb stage1/2 to amp_C python

[33mcommit f855f856e1fc84d236d317a0ee6884e3db519920[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Fri Aug 16 17:34:56 2019 -0700

    disable breaking test until switch to test against upstream v1.2.0

[33mcommit 2bc766ce0ee2f228e2fd2529cfa696c7ebc9bd03[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Fri Aug 16 17:13:03 2019 -0700

    add back legacy lamb code for backward comptibility now

[33mcommit 18062b69e32ba588c7409fd4c030df6fdd9477d8[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Fri Aug 16 16:34:17 2019 -0700

    clean up variance options support by all fused optimizers:
    correctly not apply bias correction to epsilon(same as recent upstream change)
    correctly not apply bias correction to weight decay(consistent with upstream AdamW)
    Make adam_w_mode for FusedAdam/LAMB, to do L2 or Weight Decay (Adam vs AdamW)
    Correct document reg_inside_moment differently from adam_w_mode in FusedNovoGrad
    Removed legacy eps_mode from FusedAdam
    Make internal math type float across fused optimizers

[33mcommit 7a219aa9b350d055b8bb37fab1caab0b06a9be63[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Fri Aug 16 13:59:20 2019 -0700

    fix novograd init overflow

[33mcommit c8f9cceba7a2daafb30c1c87db2764dabfb2ee6e[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Fri Aug 16 13:33:05 2019 -0700

    add fused lamb, put lamb kernels into one file

[33mcommit 453eefa56454142f8fc788478ad511973cc0fe1b[m
Author: Christian Clauss <cclauss@me.com>
Date:   Thu Aug 15 12:12:57 2019 +0200

    Undefined name: import os for line 134

[33mcommit 66158f66a027a2e7de483d9b3e6ca7c889489b13[m
Author: Christian Clauss <cclauss@me.com>
Date:   Thu Aug 15 12:10:16 2019 +0200

    Don't forget self in Properties._update_options_dict()
    
    [flake8](http://flake8.pycqa.org) testing of https://github.com/NVIDIA/apex on Python 3.7.1
    
    $ __flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics__
    ```
    ./apex/amp/frontend.py:34:21: F821 undefined name 'self'
                if k in self.options:
                        ^
    ./apex/amp/frontend.py:35:17: F821 undefined name 'self'
                    self.options[k] = v
                    ^
    ./tests/L0/run_mixed_adam/test_fp16_optimizer.py:133:19: F821 undefined name 'os'
        script_path = os.path.dirname(os.path.realpath(__file__))
                      ^
    ./tests/L0/run_mixed_adam/test_fp16_optimizer.py:133:35: F821 undefined name 'os'
        script_path = os.path.dirname(os.path.realpath(__file__))
                                      ^
    ./docs/source/conf.py:210:5: F821 undefined name 'List'
        # type: (List, unicode, Tuple) -> nodes.field
        ^
    ./docs/source/conf.py:210:5: F821 undefined name 'unicode'
        # type: (List, unicode, Tuple) -> nodes.field
        ^
    ./docs/source/conf.py:210:5: F821 undefined name 'Tuple'
        # type: (List, unicode, Tuple) -> nodes.field
        ^
    7     F821 undefined name 'self'
    7
    ```
    __E901,E999,F821,F822,F823__ are the "_showstopper_" [flake8](http://flake8.pycqa.org) issues that can halt the runtime with a SyntaxError, NameError, etc. These 5 are different from most other flake8 issues which are merely "style violations" -- useful for readability but they do not effect runtime safety.
    * F821: undefined name `name`
    * F822: undefined name `name` in `__all__`
    * F823: local variable name referenced before assignment
    * E901: SyntaxError or IndentationError
    * E999: SyntaxError -- failed to compile a file into an Abstract Syntax Tree

[33mcommit aee5aff46137954e3a0cfeb6ea973c81e022d8bf[m
Merge: 007c594 880ab92
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Tue Aug 13 14:33:45 2019 -0700

    Merge branch 'master' into deyuf/fused_optimizer_v2

[33mcommit 007c59472a94c69683b9315ee6844bcde1b15997[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Tue Aug 13 14:08:03 2019 -0700

    Reverse to Fused* naming, clean up accordingly:
    FusedSGD now work as before
    FusedAdam now work with o1/o2, no longer fuse scaling and casting
    Removed special backend handling for FusedAdam
    Moved and updated test for FusedAdam into run_optimizers
    Removed legacy tests for optimizers.FP16_optimizer and FusedAdam in run_mixed_adam

[33mcommit 880ab925bce9f817a93988b021e12db5f67f7787[m
Author: Marek Kolodziej <mkolod@gmail.com>
Date:   Tue Aug 13 09:21:42 2019 -0700

    Adding PyProf to Apex (#404)
    
    Co-authored-by: Aditya Agrawal <aditya.iitb@gmail.com>
    Co-authored-by: Marek Kolodziej <mkolod@gmail.com>

[33mcommit 37a1c121da701467089d58493c34954b049add13[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Mon Aug 12 17:41:23 2019 -0700

    add multi-precision support for novograd, clean import

[33mcommit 8599b85408d5fdbb39058d1c93c82fd42928fb6d[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Mon Aug 12 15:45:13 2019 -0700

    converge fused_sgd and sgd code(dtype support, fused kernel, wd_after)

[33mcommit adad599616bc14892f4ebf19948e07a9bd6e0ee2[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Mon Aug 12 13:45:21 2019 -0700

    keep old fused* name and rename new optimizers without prefix

[33mcommit 4d6ed501f41b45ce55bc7c2e39a28ad38fe75b76[m
Merge: 690b1f7 9f64bf2
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Mon Aug 12 11:28:03 2019 -0700

    Merge branch 'multi_tensor_sgd' into deyuf/fused_optimizer_v2

[33mcommit 690b1f718d33a4bf2f27e2be95e414793d7186d6[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Thu Aug 8 16:59:46 2019 -0700

    initial commit to make fused optimizers compatible with AMP

[33mcommit 4a8c4ac088b6f84a10569ee89db3a938b48922b4[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Wed Aug 7 19:34:12 2019 +0200

    Add DCGAN example (#413)
    
    * initial commit
    
    * add default O1 mode, enable other modes, add README
    
    * add carilli's review suggestions to README

[33mcommit 3ef01faef2492b3e650f44ecc510f3a8f2426783[m
Author: ngimel <ngimelshein@nvidia.com>
Date:   Tue Aug 6 09:30:51 2019 -0700

    Clean up layer norm tests (#418)
    
    * Bug fix for non-affine layer-norm + add backward unit test
    
    * clean up tests and add tests for a large batch

[33mcommit 37795aac0d581918ccc33dc64c6480df74b82985[m
Merge: 3d01e4a 4a8e1a8
Author: ngimel <ngimelshein@nvidia.com>
Date:   Thu Aug 1 14:58:37 2019 -0700

    Merge pull request #412 from ngimel/fix_layer_norm
    
    fix fused layer norm for >65535 batch

[33mcommit 4a8e1a876437096c5fd917e919e49ebbec32c797[m
Author: Natalia Gimelshein <ngimelshein@nvidia.com>
Date:   Thu Aug 1 14:30:16 2019 -0700

    fix fused layer norm for >65535 batch

[33mcommit 3d01e4a0a188cc8df54bc6e44cf5eb40ff6b4cc5[m
Merge: 574fe24 0dbf6c2
Author: ngimel <ngimelshein@nvidia.com>
Date:   Wed Jul 31 16:24:23 2019 -0700

    Merge pull request #400 from myleott/new_function_api
    
    Update FusedLayerNorm for new function API

[33mcommit 0dbf6c2aa203e259f221be8d245690aa5601547c[m
Author: ngimel <ngimelshein@nvidia.com>
Date:   Wed Jul 31 16:24:01 2019 -0700

    use ctx.eps instead of eps

[33mcommit 9f64bf27802eb2d3325cf709f500e0e3275d7996[m
Author: Syed Tousif Ahmed <syed.ahmed.emails@gmail.com>
Date:   Sat Jul 27 17:20:47 2019 -0700

    Add backward compatibility for ATen/Type.h

[33mcommit e86f986dafc2ec2a5f1d3c788f9b2f3575fa0645[m
Author: Syed Tousif Ahmed <syed.ahmed.emails@gmail.com>
Date:   Fri Jul 26 18:20:48 2019 -0700

    Put parser in a function to make script importable

[33mcommit 8d0deb092fe1faef073361d5550438c521c44ddf[m
Author: Syed Tousif Ahmed <syed.ahmed.emails@gmail.com>
Date:   Fri Jul 26 14:24:49 2019 -0700

    Remove deprecated Type.h

[33mcommit 896ecdd6df44deaf208b529c483b5bc8a118af45[m
Author: jjsjann123 <jiej@nvidia.com>
Date:   Fri Jul 12 15:27:04 2019 -0700

    [sbn update] (#384)
    
    fixing empty return from python implementation
      adding proper test to verify functional correctness for python implementation

[33mcommit 3f7f5fba82bd31126959cf41e933c22b23ab1399[m
Author: Edward Z. Yang <ezyang@mit.edu>
Date:   Fri Jul 12 10:35:39 2019 -0400

    Add missing semicolon. (#390)
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>

[33mcommit d1626ccca4e25a4c4cd3e27aab5be86213a7c438[m
Author: Myle Ott <myleott@fb.com>
Date:   Sat Jul 20 10:37:06 2019 -0700

    Update FusedLayerNorm for new function API

[33mcommit a00952bc07a9d7eb4f9f0ac91e3d0dab59943930[m
Merge: 1483f22 89ae9e5
Author: jjsjann123 <jiej@nvidia.com>
Date:   Fri Jul 12 17:23:57 2019 -0700

    Merge pull request #391 from NVIDIA/persistent_sync_bn_group8_fix
    
    Fixing rank mapping for bn_group size == 8

[33mcommit 89ae9e543fd8e508782f93b720717d5a1ffb7424[m[33m ([m[1;31morigin/persistent_sync_bn_group8_fix[m[33m)[m
Author: Jie <jiej@nvidia.com>
Date:   Fri Jul 12 17:21:25 2019 -0700

    Fixing rank mapping for bn_group size == 8

[33mcommit 574fe2449cbe6ae4c8af53c6ecb1b5fc13877234[m
Author: jjsjann123 <jiej@nvidia.com>
Date:   Fri Jul 12 15:27:04 2019 -0700

    [sbn update] (#384)
    
    fixing empty return from python implementation
      adding proper test to verify functional correctness for python implementation

[33mcommit 80e0143e20c4addac2837440b77852f9b99b6538[m
Author: Edward Z. Yang <ezyang@mit.edu>
Date:   Fri Jul 12 10:35:39 2019 -0400

    Add missing semicolon. (#390)
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>

[33mcommit 665b2dd7dc9d5129d7541bad612c1d86ba4b6818[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jul 3 22:43:19 2019 +0000

    Pulling in deprecation warning changes

[33mcommit d352d44099f4f9732c2077c071fa633c1023d7de[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jul 3 16:13:02 2019 -0700

    Bumping container version for test

[33mcommit 816813f9cb626166b7f446a8ca3a3181b509e49f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jul 3 22:49:15 2019 +0000

    Remove deprecated Type.h

[33mcommit 1483f22d53a37555ea1a41278cc8cb4b9537991c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jul 3 16:13:02 2019 -0700

    Bumping container version for test

[33mcommit 7096b1b75dab9bfd3c842699089420854f8365e6[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jul 3 22:49:15 2019 +0000

    Remove deprecated Type.h

[33mcommit adee29f690e1fe2d45449f179ae2b6d17e497dfe[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jul 3 22:43:19 2019 +0000

    Changing AT_CHECK to TORCH_CHECK

[33mcommit b9336b1e337cff9db20a458d9e4250252288d1ca[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jul 3 21:46:41 2019 +0000

    Fix use of multi_tensor_l2norm, remove test using deprecated syntax

[33mcommit d6271729cd1e9c8822dbe416916f0ea4a01a85cd[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Jul 3 13:41:35 2019 -0700

    Fix for #181 (add dot to CAST list)
    
    https://github.com/NVIDIA/apex/issues/181#issuecomment-507958143

[33mcommit 47da14a095e87bcd5b1ba176ddb93dd71521b9b7[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Jul 2 15:59:35 2019 -0700

    cosmetic

[33mcommit 8a32e4288805af39bb43ea0dd7844a3f77e7327a[m
Merge: d9c887c 18f2eae
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Jul 2 15:42:48 2019 -0700

    Merging in master

[33mcommit 14fe2b67d96ea5c7791263baf7f1de2b29613055[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Jul 1 17:09:02 2019 -0700

    Update scaler.py

[33mcommit ff6b8bb093de0906f7acceb07460ca902803967d[m
Merge: 18f2eae 3aeea0d
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Fri Jun 28 16:39:37 2019 +0200

    Merge pull request #383 from NVIDIA/lamb_add_fp16_support_update_term
    
    Add support for fp16 update term (new UPD_T typename in template)

[33mcommit 3aeea0d8fc3b2298606bea847d99d1b82d1cad79[m[33m ([m[1;31morigin/lamb_add_fp16_support_update_term[m[33m)[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Fri Jun 28 07:35:46 2019 -0700

    Add support for fp16 update term (new UPD_T typename in template)

[33mcommit 18f2eaeea5e40d9cde7df2523855fcb6b7e0b1d9[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jun 26 17:01:20 2019 -0700

    Clarifying documentation on gradient accumulation

[33mcommit 656d14b0c9792a1bcdc255b473dc2d6145d026ff[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Jun 25 10:21:16 2019 -0700

    grid_sample should be fp32 for now

[33mcommit 9ce80178cd5635d4e993023f1636c8178ebb8985[m
Merge: f855756 f17cd95
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Jun 24 15:19:37 2019 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit f855756961396c382826ba8b8078a081a7363fd8[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Jun 24 15:19:27 2019 -0700

    Docstring for multiple losses

[33mcommit f17cd953096069754ed631f2672ea3f725598e5d[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Jun 24 10:24:42 2019 -0700

    Update README.md

[33mcommit ca35aa79e2989cbdc51ce2781bf32dcf267af322[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Jun 24 09:47:04 2019 -0700

    Updating gradient accumulation guidance

[33mcommit f29b3f8d3859b8249f15e4835c1f485e4c841ffc[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 21 15:03:38 2019 -0700

    Make main_amp.py more profiling-friendly

[33mcommit 4b9858ec346a2020ac4c0c2c1c7abd684d2481a4[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 21 11:25:55 2019 -0700

    Don't need to blacklist mean for pytorch >= 1.1

[33mcommit 90e5b05a2d2ff3e1f59328bc284aeff5d4abe951[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 20 14:12:26 2019 -0700

    Fix end-of-epoch with record_stream

[33mcommit 1ccaaf4a8af9710bc0548ed68df05999bc48bec0[m
Merge: 68c850d cd6e46c
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Jun 18 19:09:22 2019 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 68c850d370fe18ad5aa4c4d1850692bccb9b26eb[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Jun 18 19:09:12 2019 -0700

    Fix for https://github.com/NVIDIA/apex/issues/361

[33mcommit cd6e46c2b142355b7930fb38e7761478cbe1d726[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Tue Jun 18 10:14:34 2019 -0700

    Give custom to method a higher priority (#364)

[33mcommit d5e2bb4bdeedd27b1dfaf5bb2b24d6c000dee9be[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Jun 17 17:26:45 2019 -0700

    Fix rare caching allocator race condition in imagenet prefetcher

[33mcommit d9c887c25a37a5e89877b05d2d77ca13e36e6595[m
Merge: d68ec71 333e53f
Author: jjsjann123 <jiej@nvidia.com>
Date:   Mon Jun 17 11:22:03 2019 -0700

    Merge pull request #360 from NVIDIA/gbn_update
    
    update gbn

[33mcommit c3bcf18edbb5106313099e9af14db4bd0980751d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Jun 17 11:20:44 2019 -0700

    More helpful message for unexpected opt_level

[33mcommit 333e53f71486daed204fbea7b292985f9cac0c2e[m[33m ([m[1;31morigin/gbn_update[m[33m)[m
Author: Jie <jiej@nvidia.com>
Date:   Mon Jun 17 11:20:05 2019 -0700

    commenting on unnecessarily exposed buffer at user code

[33mcommit a9f5f711a9bf7dc93ad3879a84f70a1f40b46520[m
Merge: 41c9851 121a250
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 14 13:34:46 2019 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 41c98511da603641ab02891d26896a0c15ce6e99[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 14 13:33:01 2019 -0700

    Removing gradient_average_split_factor

[33mcommit 121a25006cd085212c50c17a2ca5c5e37daed004[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Jun 13 21:00:01 2019 -0500

    Separate LDG/STG from compute loop (#359)

[33mcommit 2b67bc3309eb13f6a89da01c395386b7c0eef67d[m
Author: Evgeni Krimer <ekrimer@nvidia.com>
Date:   Thu Jun 13 21:40:52 2019 -0400

    cleanup

[33mcommit 4a9c2a5340d565c145b2d66511a5d18e79d983ba[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 13 18:38:47 2019 -0700

    Adding delay_overflow_check=False ninja control point

[33mcommit 9744e81332d568a6b01237d01de840cc2e24c118[m
Author: Evgeni Krimer <ekrimer@nvidia.com>
Date:   Thu Jun 13 21:37:02 2019 -0400

    clean dead code

[33mcommit 0ef439b6a8d652ceac98a51b5c16538474487cd1[m
Author: Evgeni Krimer <ekrimer@nvidia.com>
Date:   Thu Jun 13 21:25:30 2019 -0400

    update gbn

[33mcommit ae7f0def90f5c4c1b0c12afe16c75f8c09b4daeb[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 13 09:20:51 2019 -0700

    disable_allreduce -> _disable_allreduce

[33mcommit 1c2ba8904121e2cbc3348576d2a07d5f4746f97f[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Thu Jun 13 11:15:24 2019 -0500

    Add option to turn on/off allreduce in DDP (useful for gradient accumulation) (#356)

[33mcommit 47e3367fcd6636db6cd549bbb385a6e06a3861d0[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Jun 11 13:51:17 2019 -0700

    Allow multi_tensor_lamb to update fp16 params

[33mcommit 0466713933de1a2b5e1760d2818c0a8076adea46[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 7 10:09:40 2019 -0700

    Fix for https://github.com/NVIDIA/apex/issues/344

[33mcommit 1dca16cc9c53d47844f33cd838471de6c490792f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 6 15:55:28 2019 -0700

    Making O1 the default opt level

[33mcommit b82c6bd7613ffb9c4ea68e7306fa83aabe9fa9b5[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Jun 3 17:28:01 2019 -0700

    Adding min_loss_scale and max_loss_scale arguments to amp.initialize

[33mcommit 8be5b6bedead620db636516d064db39f82052e01[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Fri May 31 13:45:55 2019 -0500

    Multi tensor lamb optimizer (#334)
    
    * First draft, for discussion
    
    * Fix mistakes in LAMB equations
    
    * Add loop over chunk
    
    * Bug fix
    
    * Bug fix
    
    * Bug fix
    
    * Undo bug fix
    
    * Bug fix
    
    * Add multi tensor LAMB optimizer to setup.py
    
    * Rename step_size to learning_rate
    
    * Fix compilation errors

[33mcommit 93338e624c57a8e565bc7c5da07753bfab683092[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri May 31 09:22:38 2019 -0700

    Give multi-tensor L2 norm the ability to compute norms per-tensor as well as globally (#333)
    
    * Existing tests passing, still need to add per-tensor tests
    
    * Test is passing, still need to measure performance
    
    * ILP for l2norm functor

[33mcommit a151575c6c0003d3c18ac299280274b9f7253fe1[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue May 28 14:03:39 2019 -0700

    Fix for https://github.com/NVIDIA/apex/issues/332

[33mcommit d68ec712e68477dce49a0e4fa4df94f3a301a7cd[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon May 27 14:38:42 2019 -0700

    test cleanup

[33mcommit 848c777dfeb01867998755401a6d4b263b3e9fa4[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon May 27 14:32:59 2019 -0700

    FusedSGD tests passing for all opt_levels

[33mcommit 14e34f7f89967dcbe5876b8bf416e311dd90b9dd[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Fri May 24 17:12:58 2019 +0200

    add backwards compatibility for PyTorch 0.4 for named_buffers (#331)

[33mcommit c142714ba4a3485036aab2e0ef9d87aa67827d46[m
Merge: b620f96 e6eec3b
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu May 23 12:53:26 2019 -0700

    Merging in latest master changes

[33mcommit b620f96be45101c1b7344d91142179a354bee25f[m
Merge: 8ae6310 0c74571
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu May 23 11:39:51 2019 -0700

    Merge branch 'integrate_label_smoothing' into multi_tensor_sgd

[33mcommit e6eec3ba775c7d9e0ecf67ec729651d5733c65f8[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu May 23 10:45:00 2019 -0700

    Changing error message

[33mcommit 50689f6ae088f8e8c516b7f581d695969d622e38[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed May 22 16:34:47 2019 -0700

    Hard error on Pytorch Cuda + Cuda toolkit version mismatch (#323)

[33mcommit ccffa71cc566ab40e3be59743b8a10c9efc1b845[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed May 22 08:44:16 2019 -0700

    Fixing second line for 321.

[33mcommit 9bd61cc1e05a6ab9aeceb1a450c0c48c2cf002b9[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Wed May 22 17:12:56 2019 +0200

    use value in assert statement (#321)

[33mcommit c490bd368faafb19986f857f9baa145d30872e92[m
Author: blisc <jasonli9@live.ca>
Date:   Mon May 20 17:06:11 2019 -0700

    Enable LARC for use with amp (#306)
    
    * update larc
    
    Signed-off-by: Jason <jasoli@nvidia.com>
    
    * scale_loss fix
    
    Signed-off-by: Jason <jasoli@nvidia.com>
    
    * typo
    
    Signed-off-by: Jason <jasoli@nvidia.com>
    
    * revert LARC

[33mcommit a52890676f9a26a3a9bc5b8e388fd87375fc4bfd[m
Author: jjsjann123 <jiej@nvidia.com>
Date:   Fri May 17 14:54:30 2019 -0700

    [syncbn update] (#287)
    
    update input size check to fix github issue #262
    
    update SyncBatchNorm count check so that size 1 input with cross GPU
    synchronization runs fine.

[33mcommit ffbb52ba6c57091f744fe007bb0e90d9e7b7045a[m
Author: jjsjann123 <jiej@nvidia.com>
Date:   Fri May 17 14:54:02 2019 -0700

    [SyncBatchNorm update] (#285)
    
    resolves issue #254
    
    Added input casting for pure python implementation, this supports mismatched
    input and layer dtype.

[33mcommit 4d325d2f25ec0c6a1e649ee2fee2170972f32c0a[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed May 15 21:00:18 2019 -0700

    Support add_param_group (#310)
    
    * Support add_param_group
    
    * syntax
    
    * Test added and passing

[33mcommit cfb628ba2e46c9b8dc1368d6429031bbfa7a6f10[m
Author: Michael Glass <35044941+michaelrglass@users.noreply.github.com>
Date:   Wed May 15 14:16:09 2019 -0400

    use verbose parameter to control print of grad overflow (#300)

[33mcommit a3169768fcc3d3f4eba0a89fe864894ffb97160f[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Wed May 15 20:14:19 2019 +0200

    raise exception if cudnn is disabled (#305)

[33mcommit df099a4b85673d7b9579b9fd3febcd72d9bdad0c[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Wed May 15 19:40:38 2019 +0200

    fix URLs in docs of apex.parallel (#309)
    
    * fix URLs
    
    * Update distributed.py

[33mcommit f2b3a62c8941027253b2decba96ba099f611387e[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon May 13 15:02:34 2019 -0700

    Adding docker build test for 1.1 container

[33mcommit 8ae63102319ffbecd491dda298c6543df2252270[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon May 13 11:17:24 2019 -0700

    Progress towards materialize_master_grads=False

[33mcommit 54b8a852a9d2b9da527c5f4f747654be6a607f46[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon May 13 10:35:28 2019 -0700

    Fix for #302

[33mcommit c763f0fe50f3541d614e916f598c44f185e5c0a0[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu May 9 19:56:18 2019 -0700

    materialize_master_weights for FusedSGD

[33mcommit 4ff153cd50e4533b21dc1fd97c0ed609e19c4042[m
Author: Tim Zaman <timbobel@gmail.com>
Date:   Thu May 9 12:49:02 2019 -0700

    Fix link to distributed samples (#298)

[33mcommit 0c74571f3407bf020a1b65a70548e20f34c62162[m[33m ([m[1;31morigin/integrate_label_smoothing[m[33m)[m
Author: Wil Kong <alpha0422@gmail.com>
Date:   Fri May 10 02:30:28 2019 +0800

    Add softmax cross entropy loss with label smoothing support. (#295)
    
    * Add softmax cross entropy loss with label smoothing support.
    
    * Fix deprecation of AT_DISPATCH_XXX and several minor issues.
    
    * Fix issues commented by reviewers.
    
    * Add FB license.
    
    * Remove code generation constraints.
    
    * Add a simple unittest for label smoothing.

[33mcommit 3e2883ddd81daf8dee26a3d1a427df22559006de[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue May 7 20:07:33 2019 -0700

    Checkpoint fix

[33mcommit f3528d991df297255e21026f595cff3535a1e9ea[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri May 3 14:30:10 2019 -0700

    Converting dispatch macros in fused_adam_cuda_kernel.cu

[33mcommit d050543389fa86e582340d389b115c68e1036fbb[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu May 2 13:21:41 2019 -0700

    Adding test_fused_sgd.py

[33mcommit 00dbe4b440d7b0cbf7942672f0b1efde79031711[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu May 2 10:28:55 2019 -0700

    test_fused_sgd.py passing

[33mcommit 72bce160dc6125a059ed2d3642d3eb104567b871[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed May 1 09:13:57 2019 -0700

    allreduce_different_streams is now hidden

[33mcommit 3b4a0a23f7e963bb08148c95552eaa76bcd8309d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed May 1 09:03:58 2019 -0700

    Fixing syntax

[33mcommit 719578361fcb51ac29be15912e396ec061e53084[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed May 1 08:46:19 2019 -0700

    Option to preinitialize allreduce communicators

[33mcommit 39e153a3159724432257a8fc118807b359f4d1c8[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 30 11:11:53 2019 -0700

    Clarifying docker launch

[33mcommit 86bd6c79decc9c7aba47f21855d3ead1d686e3b4[m
Merge: d2ac487 ca2baff
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 30 10:59:00 2019 -0700

    resolving delete conflicts

[33mcommit d2ac4872fe7b9412400a36b47afe6ff515fa7223[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 30 10:57:06 2019 -0700

    Remove deprecated examples and update Docker guidance

[33mcommit ca2baffbb37838e3e32261f751a730b371901347[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Tue Apr 30 17:15:00 2019 +0200

    Remove unused tensor in fast_collate (#281)
    
    * remove unused tens tensor in example/imagenet/main_amp.py
    
    * remove unused tens tensor in deprecated examples and tests/L1

[33mcommit 03a25ba8a61d15a503405ef4bbda83724bb531b2[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Apr 29 17:36:55 2019 -0700

    Casting logic should reflatten RNN parameters

[33mcommit 1b8303d8cf7e488967675e8f73f2aca52ae1bd04[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Apr 29 13:41:17 2019 -0700

    Adding warning for amp.scale_loss

[33mcommit 7b245dba17f4a3def85859f5a1a68737ea147dbf[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Apr 29 10:47:36 2019 -0700

    Warning for inception_v3

[33mcommit fedfe0d7159711198a77ca1a6ba8cc20d665ddce[m
Author: jjsjann123 <jiej@nvidia.com>
Date:   Fri Apr 26 18:10:25 2019 -0700

    Bnp integration pr (#275)
    
    * Persistent group batchnorm added
    
    Added persistent grouped batch norm for performance run on strong scaling case:
    currently only supporting:
    
      1. nhwc layout
      2. fp16
      3. synchronization only within a node!
    
    Environment variable is used to tune LAUNCH_MARGIN that limits the CTAs usage
    by the persistent kernel.
    
    Documentation and examples will follow.
    
    * updating type().scalarType() to scalar_type()
    
    * moving launch margin to be defined at layer creation, adding a knob cap max ctas per sm
    
    * fixing the cta computation
    
    * review comment:
    
    set device_id through cudaGetDevice()
    move cudaMemset to cudaMemsetAsync
    updated __threadfence() to __threadfence_system() inter device write

[33mcommit e7beba17b7025469b6c06352b38ceda509644bc9[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sat Apr 27 01:07:36 2019 +0000

    syntax

[33mcommit d175acb0d04172b60b9aa4e54a3c2a906ce36757[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Apr 26 16:59:55 2019 -0700

    Removing instances of ScalarType, still need to change macros

[33mcommit d900e93c6892fdfb34bb9267b75e49e5372aa1f2[m
Merge: c978bda 855808f
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Apr 26 16:39:11 2019 -0700

    Merging in master

[33mcommit c978bda56459915229636739dfc5eeb37e376442[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Apr 26 16:36:09 2019 -0700

    whitespace

[33mcommit 73d4212d741d3c1b1346d0ce3ef8dbdc407976c8[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Apr 26 16:25:15 2019 -0700

    Explicit control over number of allreduce streams for DDP

[33mcommit 855808f3fc268e9715d613f3c2e56469d8c986d8[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Fri Apr 26 18:17:01 2019 +0200

    Replace type().ScalarType() with scalar_type() (#272)
    
    * change .type().ScalarType() to .scalar_type() + at::ScalarType::X to at::kX
    
    * revert scalar_type() to type() for AT_DISPATCH_FLOATING_TYPES_AND_HALF
    
    * revert scalar_type() to type() in AT_DISPATCH_FLOATING_TYPES
    
    * revert scalar_type() to type() for AT_DISPATCH_FLOATING_TYPES_AND_HALF in welford.cu
    
    * revert scalar_type() to type() in layer_norm_cuda_kernel.cu
    
    * revert at::kType  to at::ScalarType::Type
    
    * use DISPATCH_FLOAT_AND_HALF to get rid of warnings
    
    * add dispatch mechanisms for double+float and double+float+half

[33mcommit 070c7e964c43f9e5c24f5208ad8e2a1cd41dcd39[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Apr 26 03:16:03 2019 +0000

    Tested on 1x8x1

[33mcommit 3b32c401aaa7e7d5c68f2d8880e1da3e6f824928[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Apr 26 02:01:58 2019 +0000

    Fixed bounds checking

[33mcommit 2c63ba910186f3932ea6a4ff75e8586ea0699ea1[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 25 16:05:00 2019 -0700

    Don't launch for empty sets

[33mcommit 91362442e508a93471bad889e61297a9ae64b8a9[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 25 13:29:32 2019 -0700

    syntax

[33mcommit 75139ca33853c88968a6781e0b15992184e78446[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 25 13:19:20 2019 -0700

    let's see

[33mcommit e0f2ffa5f378bc7b6e1a0082270fbdb343dd9c93[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Apr 24 12:38:44 2019 -0700

    Initial organization

[33mcommit bf4aa847d7a6e669bc3c9e1c2040a54fcc8dd1b0[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 23 20:57:41 2019 -0700

    Moving sgd to optimizers

[33mcommit 6af5980e7acaa715c06da8477f535686bed1b464[m
Merge: 16a3bdf 7aad54f
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 23 20:49:45 2019 -0700

    Merging in FusedAdam treatment

[33mcommit 7aad54f745b614c8307c44a058d01f0966bac05d[m[33m ([m[1;31morigin/prepare_fused[m[33m)[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Apr 24 01:40:58 2019 +0000

    Updating explanation for record_stream

[33mcommit 25ac98970c8a71a9e517f394b9858dc309bc6be0[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 23 21:21:32 2019 +0000

    Moving flat allreduce buffer creation to main stream

[33mcommit 1c464b48472e32bdb752f77815502bffe0acd284[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Tue Apr 23 19:57:37 2019 +0200

    move and fix check_optimizers (#268)

[33mcommit 16a3bdf33d4ac9712bc69cbdf8acf1cfaec4b449[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Apr 22 13:20:53 2019 -0700

    Updating TensorList->TensorListMetadata

[33mcommit 651150cb285f33541e07372819f559c9f5083cc1[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 18 15:32:57 2019 -0700

    cleanup

[33mcommit 843cdbe01a63fe2e0eca35e4d910130b1b8a5aad[m
Merge: 724672d 28097c9
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 18 15:30:45 2019 -0700

    Merging in master

[33mcommit 28097c9999d86bd889a1f03c963e29e3384f3996[m
Author: ptrblck <ptrblck@users.noreply.github.com>
Date:   Thu Apr 18 18:26:22 2019 +0200

    initial commit, add CUDA warning to check_params_fp32 (#263)

[33mcommit cd2708ccdb66e15c94564e3f608a44aaa5d6a4ab[m
Author: Glenn Jocher <glenn.jocher@ultralytics.com>
Date:   Thu Apr 18 16:53:29 2019 +0200

    Update README.md (#261)

[33mcommit b8965a78ca49a4a15863b2b4d23867f858d5188a[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Apr 17 17:12:58 2019 +0000

    Option to elide unflattening copy

[33mcommit 887a50bdf3729be9daadd802e476808604c83d17[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 16 22:30:23 2019 +0000

    Better way to expose scale adjustment

[33mcommit 9efb2809f2d9fc545669e69e312469933ed72fa5[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 16 22:23:56 2019 +0000

    Compatibility between skip_step and FusedAdam.step

[33mcommit 111ee132dadfdf666b30da37910a74f4d553ebff[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 16 22:12:26 2019 +0000

    Adding control point for scale adjustment

[33mcommit 0b5dd0208990785dd52ae0fea47a5fc578908191[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 16 10:18:03 2019 -0700

    Adding option to ensure that model outputs are a desired type

[33mcommit d69011debdfb08086b3d79232e193cc552137faa[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 16 10:18:03 2019 -0700

    Adding option to ensure that model outputs are a desired type

[33mcommit eea4c0aa61575a4b8545d88496eeddb7b4469612[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Apr 15 14:40:00 2019 -0700

    fp16_groups is an attribute of _amp_stash

[33mcommit e5213b281c1e8c62b1e097a2a189c20a93e79680[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Apr 15 14:32:36 2019 -0700

    Scaler not needed for prepare_backward*fused

[33mcommit 5ae6008daa53580e1c63777fd7e872a83e004839[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Apr 15 14:18:00 2019 -0700

    For testing purposes, enable the case where FusedAdam is not wrapped by amp

[33mcommit 53fd093d50b0ca7d60793a2b0a6ea8c329eec7e8[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Apr 12 08:43:47 2019 -0700

    Update Wil's code + typo

[33mcommit 3c53cf81fef050f151d8893e46a05f88552e0f04[m
Merge: b7f10ad 4dc711b
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 11 15:29:07 2019 -0700

    Merge branch 'master' into prepare_fused

[33mcommit b7f10ad06addeae1df29eb63ad5d65da3fa4d4dd[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 11 15:24:10 2019 -0700

    typo

[33mcommit 8521bb22c994b224803ac1fc0754e50c5d8ab068[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 11 15:16:42 2019 -0700

    Patching in changes to enable multiple allreduces in flight

[33mcommit 61b8a0fd523a7933304159240f0edcec036f6ae8[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 11 14:41:30 2019 -0700

    Rough cut, control flow should work for scaleout testing

[33mcommit 4dc711bc37e7229e7d37fb1ff358cb50991f43af[m
Author: henrymai <henrymai@users.noreply.github.com>
Date:   Thu Apr 11 16:50:30 2019 -0400

    prelu belongs in FP16_CASTS (#257)
    
    The main use of these functions (e.g.: `torch.{conv*, prelu}`) is via their `torch.nn`
    wrapping layers.
    
    The `torch.nn` layers are what contain the weights and call into these lower level
    functions with the weights as a parameter in their `forward()` method.
    
    The `torch.conv*` functions are already in the `FP16_CASTS` list due to amp's philosophy of
    casting the parameters rather than the model/layer weights.
    
    Conceptually `torch.prelu` is the same as the `torch.conv*` case, where its weight parameter
    is passed in from its wrapper layer `torch.nn.PReLU`.

[33mcommit dda59354f11e4fbcaa8aaacf9e5fde68f3f24da5[m
Merge: fc6c5a2 856f87a
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Apr 10 20:24:46 2019 -0700

    Fixing merge conflict in setup.py

[33mcommit fc6c5a25b243214f551dfd7ad9037b1cfdb7367d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Apr 10 20:18:27 2019 -0700

    some cleanup

[33mcommit 2c18651bef382c000cf00655ab79cc6aa27bcb66[m
Merge: 683b6e0 6d40465
Author: ngimel <ngimelshein@nvidia.com>
Date:   Wed Apr 10 13:20:33 2019 -0700

    Merge pull request #256 from LamDang/master
    
    quick fix: make FusedLayerNorm compatible with cpu

[33mcommit 6d40465a7a90a9e07d00cecbb17faced04a19ce6[m
Author: Lam Dang <tunglam.dang@gmail.com>
Date:   Wed Apr 10 20:09:51 2019 +0000

    add new tests to run_test.py

[33mcommit d130ec1f61f3bd0af4b18d36589c3f73ac1afd5c[m
Author: Lam Dang <tunglam.dang@gmail.com>
Date:   Wed Apr 10 20:05:31 2019 +0000

    quick fix: make FusedLayerNorm compatible with cpu

[33mcommit 683b6e0e4de3f7fb5ea4388d601de12a854e1222[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Apr 10 11:41:34 2019 -0700

    Quick kernel to clean up l2norm

[33mcommit 1a48b26b5ffab01a22033be4b00f8fac11ce653d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 9 18:04:57 2019 -0700

    Kernel + sizes stress test

[33mcommit e57f5d0e69359dbff9c2e23b49275820970fc55d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Apr 9 15:59:54 2019 -0700

    Simple cut of the kernel in place

[33mcommit 03100f466145a0de73b79c32d11d58c917b10393[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Apr 8 11:42:39 2019 -0700

    Fix for #246

[33mcommit e9bbfa591af01168f308ce8e7149252a50a75300[m
Merge: 2eccdbd dc8f400
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 4 18:09:18 2019 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 2eccdbd22e8c903585168e2bef9360954ecb444a[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 4 17:52:34 2019 -0700

    docstring

[33mcommit 0750a757e76eb5aa9edda2ffafe08f4884470dc2[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 4 17:44:33 2019 -0700

    delay_unscale is never necessary and generally discouraged, but should still work for some cases

[33mcommit dc8f400f888ddb4f13f4266cf6791fb99e513271[m
Merge: 3f87614 47e9cae
Author: ngimel <ngimelshein@nvidia.com>
Date:   Thu Apr 4 10:32:38 2019 -0700

    Merge pull request #241 from mkolod/fp32_interp
    
    Run interpolation in fp32 because it's faster

[33mcommit 47e9cae58b522ede8d7a398451abefa662e01411[m
Author: Marek Kolodziej <mkolod@gmail.com>
Date:   Thu Apr 4 10:18:43 2019 -0700

    Run interpolation in fp32 because it's faster

[33mcommit 3f87614f3e07055bf7680cdbea2340dcf10d63c5[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Apr 3 22:07:54 2019 -0700

    WIP:  Handle arbitrary combinations of optimizers/models/losses (#232)
    
    * Refactor to allow more flexible treatment of multiple optimizers/models/losses
    
    * Adding _process_optimizers.py
    
    * Created L0 tests (now passing).
    
    * fix: minor print typo (#234)
    
    * make L1 results easier to read
    
    * L0 multiple model/optimizer/loss test fleshed out
    
    * Adding test that master params remain synced across distributed processes
    
    * Docstring updates
    
    * Docstring updates

[33mcommit 214fda42dfc36d37906838ba1718d01f1c7a3d7b[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Apr 3 13:51:12 2019 -0700

    Allow verbose casting information for O1

[33mcommit 8402878650eeca3cad3d01c1c7ecb80e9a0cb1fc[m
Merge: a8c2b7d 9b114c1
Author: jjsjann123 <jiej@nvidia.com>
Date:   Mon Apr 1 10:13:35 2019 -0700

    Merge pull request #233 from DTennant/patch-1
    
    Fix a typo in optimized_sync_batchnorm_kernel.py

[33mcommit 9b114c1528ff547dbc48b4b9e020c46c371b9160[m
Author: Bingchen Zhao <Tennant_1999@outlook.com>
Date:   Sun Mar 31 12:49:54 2019 +0800

    Update optimized_sync_batchnorm_kernel.py
    
    in line 54, running_var should be running_variance..

[33mcommit a8c2b7dd12cd8dc3ceb7137581dd1af90f5ee9d5[m
Merge: f5cd5ae f1123e3
Author: ngimel <ngimelshein@nvidia.com>
Date:   Wed Mar 27 14:30:52 2019 -0700

    Merge pull request #225 from NVIDIA/bmm-fp16
    
    Conditionally run bmm functions in fp16 based on cuda version

[33mcommit f1123e3292c26be92d98e0cb88a2112991100867[m[33m ([m[1;31morigin/bmm-fp16[m[33m)[m
Author: Carl Case <carlc@nvidia.com>
Date:   Wed Mar 27 14:19:35 2019 -0700

    Conditionally run bmm functions in fp16 based on cuda version

[33mcommit f5cd5ae937f168c763985f627bbf850648ea5f3f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Mar 26 09:03:11 2019 -0700

    Minor docstring updates

[33mcommit e7f195603d97f17664b89392f1d1b80d5c85d98f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Mar 26 08:28:32 2019 -0700

    Add some links

[33mcommit dc55a996ea0232fec50a55b39fc24078b1b37b43[m
Author: Cubbee <param3456@gmail.com>
Date:   Sat Mar 23 15:46:34 2019 +0400

    Fix typo in setup.py error message on torch version check (#219)

[33mcommit 0a991543846966d5f586540dc2441e512139e9fc[m
Author: jjsjann123 <jiej@nvidia.com>
Date:   Fri Mar 22 08:37:12 2019 -0700

    [SyncBatchNorm] (#206)
    
    supporting 2 dimensional input, resolving issue #194
    
    Implementation:
      for 2d input, switching channel_last flag to true for better memory access
    pattern in the kernel.

[33mcommit 570fde7067271ae8e0a8f317bd9891d5f38d45f8[m
Author: henrymai <henrymai@users.noreply.github.com>
Date:   Fri Mar 22 11:36:32 2019 -0400

    Add prelu to list of torch overrides (#217)
    
    * Add prelu to list of torch overrides
    
    This is to fix the following error:
    
      File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
        result = self.forward(*input, **kwargs)
      File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py", line 92, in forward
        input = module(input)
      File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
        result = self.forward(*input, **kwargs)
      File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/activation.py", line 722, in forward
        return F.prelu(input, self.weight)
      File "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py", line 1040, in prelu
        return torch.prelu(input, weight)
    RuntimeError: expected scalar type Half but found Float
    
    * Update torch_overrides.py

[33mcommit ba429e51c6c0ac257d1b36d68cb054fbe400aa04[m
Author: enricoschroeder <10079627+enricoschroeder@users.noreply.github.com>
Date:   Fri Mar 22 16:11:50 2019 +0100

    Fix 'local variable 'optimizers_was_list' referenced before assignment' when amp.initialize() is called with optimizers=None (#218)

[33mcommit 5b8faa29217208998aa13e11dcdab695363ac6e5[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Mar 21 18:45:58 2019 -0700

    Check cuda version (#216)
    
    * Adding Torch + bare-metal nvcc version check and container build tests
    
    * Putting a canary in the coalmine
    
    * canary proved elusive
    
    * Trying direct setup.py install
    
    * this should work
    
    * Removing canary
    
    * hopefully this works

[33mcommit 6e5d909965064bf35e1d435eda857447b4ae8ec9[m
Merge: 823b30c 0f5e3fe
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Mar 21 13:59:35 2019 -0700

    Merge pull request #169 from NVIDIA/intlist-intarrayref
    
    Rename IntList to IntArrayRef

[33mcommit 0f5e3fe071ba3bfc26621a5e7f65fbdcdbc73af6[m[33m ([m[1;31morigin/intlist-intarrayref[m[33m)[m
Author: Syed Tousif Ahmed <syed.ahmed.emails@gmail.com>
Date:   Thu Mar 7 14:49:23 2019 -0800

    Use build macro for backward compat

[33mcommit 2a4670908182328f506071300f546db584f5292a[m
Author: Syed Tousif Ahmed <syed.ahmed.emails@gmail.com>
Date:   Fri Feb 22 17:02:18 2019 -0800

    Rename IntList to IntArrayRef

[33mcommit 823b30cfcfb7924d46aa97ebd31c44da734bb4e6[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Mar 20 14:17:02 2019 -0700

    Allow optional optimizer part 2

[33mcommit b80b4d378503cefdc3d5618fd813eb75d2bb62c6[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Mar 20 14:13:30 2019 -0700

    Allow optional optimizers part 1

[33mcommit bd0db55e2bb515c291376283b5526f30b12c7c2f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Mar 19 17:16:19 2019 -0700

    Adding documentation on custom batch casting

[33mcommit ee69ab648329dab25bde3a48cc66014efa450856[m
Merge: ac7dbf6 56de058
Author: mcarilli <mcarilli@gmail.com>
Date:   Tue Mar 19 16:10:18 2019 -0700

    Merge pull request #207 from arielai/master
    
    More permissive inputs to forward function

[33mcommit ac7dbf671f538b1e9b3e45cf7192c6f395f44367[m
Merge: 8437d29 5e55200
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Mar 19 07:15:00 2019 +0000

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 8437d29505fcc7fad28183395abd89a09a17efe6[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Mar 19 07:14:55 2019 +0000

    Fixing interaction of DDP with dynamic loss scaling

[33mcommit 5e55200404f721d54a1ac1f82877addfe425f31a[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Mar 18 23:31:36 2019 -0700

    Multi-tensor axpby kernel for more flexible unscaling (groundwork for #163 and #179 fix)

[33mcommit 56de058fac8c86f72aef9b8b43bc481772b18879[m
Author: George Papandreou <george.papandreou@gmail.com>
Date:   Sun Mar 17 12:36:06 2019 +0000

    replace print with warning for cpu tensors and allow numpy arrays as inputs to forward function

[33mcommit 74c06d8710800f63bd964362ef84075a59a93c9c[m
Merge: 8ab5e47 2c8e1c8
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri Mar 15 17:19:17 2019 -0700

    Merge pull request #205 from NVIDIA/remove_type_shim
    
    Conforming with upstream #17996

[33mcommit 2c8e1c86ca2f44e214c0045551b0c2352d02d6c9[m[33m ([m[1;31morigin/remove_type_shim[m[33m)[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Mar 15 14:36:04 2019 -0700

    Anticipating upstream #17996

[33mcommit 8ab5e4703f404294c3bed3b91fc7601f67701345[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Mar 14 10:20:42 2019 -0700

    Update README.md

[33mcommit a730f38fb2d6f93e120593005fa31d863bb49de7[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Mar 14 10:19:40 2019 -0700

    Adding simple distributed example for #200

[33mcommit 7f0d8c874ef36f0321911144f93d448c75a10505[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Mar 13 11:36:29 2019 -0700

    multi-tensor apply for DDP unflatten

[33mcommit 65ca6177e0e308059f13aef9674dfe1630ef9d2a[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Mar 13 11:05:07 2019 -0700

    Fix for #186

[33mcommit 856f87a18dd7c95669741386de13816e2d6ae818[m
Author: Wil Kong <alpha0422@gmail.com>
Date:   Wed Mar 13 00:41:51 2019 -0700

    Add unittest for multi-tensor apply FusedAdam.

[33mcommit 3f86316ef3800c943fc1ab8598553b52dd71631c[m
Author: Wil Kong <alpha0422@gmail.com>
Date:   Wed Mar 13 00:40:13 2019 -0700

    Add FusedAdam with multi-tensor apply support.

[33mcommit d1f74a3e4d0340564dd6dcf6061b3a1cacf1f3c7[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Mar 12 20:02:04 2019 -0700

    Casting model output as well as input, for #195

[33mcommit 80185371d426c1d5dce1bbc128281160d1a5925f[m
Merge: 895c516 e72283a
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Mar 12 11:49:23 2019 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 895c51670faa265fa87d5062c9a736ae8c0a8408[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Mar 12 11:49:10 2019 -0700

    Clarifying docs on gradient clipping

[33mcommit e72283add8563be4854d985e1feb74cf456fa835[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Mar 11 22:20:44 2019 -0700

    Update README.md

[33mcommit d27a321abd3ec511bcf4f356958010e201142556[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Mar 12 04:57:30 2019 +0000

    Moving test_groups.py

[33mcommit 42180bd929639c137706be99c93626da93a36f2e[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Mar 11 21:38:18 2019 -0700

    Forward/backward compatibility around pytorch 3aeb78, to fix #191

[33mcommit 975ed32285b93dc58e35911d1174fd36ed896382[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Mar 11 09:31:17 2019 -0700

    Fix #193

[33mcommit 724672d768ffbd716d6f7d5ba1982fb6e08a5c66[m
Author: Simon Layton <slayton58@gmail.com>
Date:   Mon Mar 11 08:52:20 2019 -0700

    Fix momentum initialization with weight decay

[33mcommit b265b0b50600e671c5af7a94799b3b9f4db4df9e[m
Author: Simon Layton <slayton58@gmail.com>
Date:   Mon Mar 11 07:46:48 2019 -0700

    Fix dispatch, add wd after momentum option
    
    Fix dispatch where we have a parameter group with multiple
    combinations of types
    Optionally apply weight decay after momentum

[33mcommit 1be97ccea062e8c42c2bc8102d2b56f05ea8d4f5[m
Merge: 8f53411 f34686f
Author: ngimel <ngimelshein@nvidia.com>
Date:   Sat Mar 9 18:04:03 2019 -0800

    Merge pull request #190 from ngimel/fix_includes
    
    fix includes

[33mcommit f34686f160f62cb7a9aed63756d589e4bfb28c81[m
Author: Natalia Gimelshein <ngimelshein@nvidia.com>
Date:   Sat Mar 9 17:53:42 2019 -0800

    fix includes

[33mcommit 8f53411a4b1252a580d7065e9a59306b64253476[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Mar 10 01:31:34 2019 +0000

    Removing deprecated scale_check_overflow kernel

[33mcommit ac74f3452e5205fae10362c4745eeb2847dddec7[m
Author: Simon Layton <slayton58@gmail.com>
Date:   Fri Mar 8 19:24:45 2019 -0800

    Fix momentum in non-nesterov case

[33mcommit 62ce27d2e90d51b08e2ed28d553f5b3e44560ccd[m
Merge: e305373 2f0bf59
Author: ngimel <ngimelshein@nvidia.com>
Date:   Fri Mar 8 18:06:54 2019 -0800

    Merge pull request #182 from FDecaYed/deyuf/update_norm
    
    Remove LoadLibrary half norm hack

[33mcommit e3053736541a1a7c6b78d19c2a419b2483e65a2c[m
Merge: 40555b3 a3a09c8
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri Mar 8 15:11:28 2019 -0800

    Merge pull request #189 from NVIDIA/verbosity
    
    Route amp print statements through maybe_print

[33mcommit a3a09c8c814158ec5b1ed2aa0791a93f9d339577[m[33m ([m[1;31morigin/verbosity[m[33m)[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Mar 8 23:07:33 2019 +0000

    Fix for #188

[33mcommit 371633d5e023a19d6f36be1267e545660e331a3d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Mar 8 21:50:36 2019 +0000

    Repr for import error

[33mcommit 6d6f0bc279aeff2a830be99199ce95661162f541[m
Author: Simon Layton <slayton58@gmail.com>
Date:   Fri Mar 8 13:35:10 2019 -0800

    Simplify noop exit condition

[33mcommit 59e992da71b678fbdbeb6181f8dd8fd81bdc495b[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Mar 8 12:18:21 2019 -0800

    Stashing to test on the cluster

[33mcommit 2f0bf5946c45a07e23842737a8742d672d022942[m
Merge: 9949537 40555b3
Author: Deyu Fu <Deyu.Foo@gmail.com>
Date:   Fri Mar 8 11:14:26 2019 -0800

    Merge branch 'master' into deyuf/update_norm

[33mcommit a2799893b7b1fc2ef1112551e59120e0e1239a15[m
Author: Simon Layton <slayton58@gmail.com>
Date:   Fri Mar 8 10:34:44 2019 -0800

    Handle fp16 weights case without forcing fp16 math
    
    Incorrect types used in a few places

[33mcommit 75c8a97a3c4bc889875ea7f9a56dcf89f817ac30[m
Author: Simon Layton <slayton58@gmail.com>
Date:   Fri Mar 8 10:15:24 2019 -0800

    Simplify C++-side logic
    
    Only support the 4 specific cases we care about
    Remove more general set of switch statements

[33mcommit cac061a10d953372e4dffd3b5b5b021cf70f58f4[m
Author: Simon Layton <slayton58@gmail.com>
Date:   Fri Mar 8 09:23:33 2019 -0800

    Code cleanup, add fused fp16 read / write
    
    Fuse in fp16 gradient -> fp32 convert
    Additional option fp16 weight copy written out

[33mcommit cadad920fb99e7e21c475e6780c4739e08b75414[m
Author: Simon Layton <slayton58@gmail.com>
Date:   Fri Mar 8 08:18:45 2019 -0800

    Fused multi-tensor SGD
    
    Initial implementation, all fp32
    Tested against torch.optim.sgd

[33mcommit 40555b3a5e5ef8faefed69cb599fad73afdb9574[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Mar 7 15:39:59 2019 -0800

    Update README.md

[33mcommit a74a0f201f79e3f48611298e9c03bb2a2a9488e3[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Mar 7 15:39:04 2019 -0800

    Update README.md

[33mcommit d2862d5ab836c80ec3d64f1c77fc8442303b78eb[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Mar 7 15:34:43 2019 -0800

    Update README.md

[33mcommit 589328ff4c2e625fceac828ce9382380d67106fd[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Mar 7 13:55:54 2019 -0800

    Support for custom batch types

[33mcommit 533e88d7c2d8430f998bba15be4b0e4973407125[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Mar 7 11:55:15 2019 -0800

    More rearrangement

[33mcommit a21e633af51c5693520c106dff48e1c566d0b51d[m
Merge: 4606df9 cfa5a65
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Mar 7 11:33:32 2019 -0800

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 4606df98af28a504930fa69357cd429ba20aefe5[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Mar 7 11:33:28 2019 -0800

    Rearranging documentation

[33mcommit cfa5a6570190a569b5a9df94477ca4506de12274[m
Merge: 0c2a629 1989a57
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Mar 7 08:30:37 2019 -0800

    Merge pull request #184 from justusschock/master
    
    Add "dynamic" argument to loss scaler

[33mcommit 1989a57548f3ce974202a1657a2bedb31769e901[m
Author: Justus Schock <12886177+justusschock@users.noreply.github.com>
Date:   Thu Mar 7 17:01:21 2019 +0100

    Add "dynamic" argument to loss scaler

[33mcommit 0c2a629da825fdd7fbbb57fee9c57443db6e7f2f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Mar 6 19:04:02 2019 -0800

    Adding advanced.rst

[33mcommit 47144979b3a348b06e06ebbb7eccfd375936aef5[m
Merge: 1603407 6644c6e
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Mar 6 18:55:18 2019 -0800

    Merge pull request #173 from NVIDIA/api_refactor
    
    Unified mixed precision API + backend performance improvements

[33mcommit 6644c6e6cacf42fb0d14f1e9ff2bd11220c9ec1b[m[33m ([m[1;31morigin/api_refactor[m[33m)[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Mar 6 18:40:54 2019 -0800

    Converging

[33mcommit 99495376ec4f0bfe68f7b82c2d55a1a7246a18b9[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Wed Mar 6 17:10:54 2019 -0800

    make norm compatible with pytorch version <= 1.0.1

[33mcommit 248d7b10661e9813793c435d2efbbfb0694f2c6b[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Mar 6 16:31:02 2019 -0800

    Updating error checking on property overrides

[33mcommit 9466741775766b0c2f39da7400c4239894343c75[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Wed Mar 6 14:56:59 2019 -0800

    change to use now great torch.norm

[33mcommit d44ce75af177c8e87c1ca4dcede375528b3ed753[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Mar 6 13:58:10 2019 -0800

    Documentation updates

[33mcommit 7f39db93317e75d5cc7dffbb7eb0e10bce27b22c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Mar 4 21:03:23 2019 -0800

    Documentation updates

[33mcommit df83b67e312742f5a1e2987c8d4ebe77a18c71b1[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Mar 4 15:48:20 2019 -0800

    Cleaning up READMEs

[33mcommit 1603407bf49c7fc3da74fceb6a6c7b47fece2ef8[m
Merge: 603e17a ca6c276
Author: jjsjann123 <jiej@nvidia.com>
Date:   Mon Mar 4 09:39:12 2019 -0800

    Merge pull request #176 from NVIDIA/next_pow2_fix
    
    Bug fix in next power of 2

[33mcommit 6066ddd575d54d84b084a28e3a11929a0a7b403f[m
Merge: b90b057 603e17a
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Mar 3 21:23:03 2019 -0800

    Syncing with master

[33mcommit b90b0570926083771ac838cc7906f31d88ba9f49[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Mar 3 21:06:27 2019 -0800

    Minor typos

[33mcommit a3dbea38c46a3cc8dc8a8757fb640389931ab0a4[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Mar 3 20:37:43 2019 -0800

    Adding summary

[33mcommit 26b30d13970d292158d8269615e82824168305cc[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Mar 3 20:28:26 2019 -0800

    README touchup

[33mcommit 3c4b789057502345903aa5ffe92f1730d55aebbb[m
Merge: 254cad2 a128fa5
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Mar 3 19:45:56 2019 -0800

    Merge branch 'api_refactor' of https://github.com/NVIDIA/apex into api_refactor

[33mcommit 254cad2dc89f9fe5cf61e27e7121b15838e0ad84[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Mar 3 19:45:39 2019 -0800

    Comprehensive imagenet example for the new API

[33mcommit a128fa54784eb9f616b7f09de75e203e087e9854[m
Merge: 484292f 519ff81
Author: mcarilli <mcarilli@gmail.com>
Date:   Sun Mar 3 15:09:19 2019 -0800

    Merge pull request #171 from vfdev-5/patch-1
    
    Fix minor typo

[33mcommit ca6c2760e20f98c094096cf42fa00b9ff0833cf6[m[33m ([m[1;31morigin/next_pow2_fix[m[33m)[m
Author: Marek Kolodziej <mkolod@gmail.com>
Date:   Sun Mar 3 14:46:19 2019 -0800

    Bug fix in next power of 2

[33mcommit 484292f0663affb07e8905927a630139c966263d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sat Mar 2 20:30:39 2019 +0000

    some test cleanup

[33mcommit 603e17a5eef8d41903742e21bc14881932436a3f[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri Mar 1 10:07:48 2019 -0800

    Update README.md

[33mcommit 2c6e64907aa2b8bd96f51ae742241e2846c86dc2[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Feb 28 20:10:32 2019 -0800

    Update README.md

[33mcommit 2445031d2620dfac86595f5dc30f61ea6b53b5e7[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Feb 25 14:24:37 2019 -0800

    Cherry picking RNN fix

[33mcommit 612d419382d416eb2d9c66190fc44c55206e08db[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Feb 28 19:10:39 2019 -0800

    Moving common test stuff to common

[33mcommit 7c82f221e584b3f3b9c48984f46ff7f6f76f3a1d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Feb 28 18:42:41 2019 -0800

    Cleaning up FusedAdam testing

[33mcommit d8b5d1bef2affcec5228d0ed3a04ea47a8ae7cba[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Feb 28 17:21:09 2019 -0800

    Adding distributed tests and support for FusedAdam

[33mcommit 519ff816c41b56a1a7f85e45e8bbbedfbd6d4329[m
Author: vfdev <vfdev.5@gmail.com>
Date:   Thu Feb 28 16:26:37 2019 +0100

    typo

[33mcommit d24c25b92ad05281f018aa489122756afdf3607d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Feb 27 21:20:06 2019 -0800

    Comprehensive tests for cross product of options

[33mcommit 613997ea871c582b6e2e8fded1d8e89abf598b18[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Feb 26 12:38:51 2019 -0800

    No need for casts during optimizer step

[33mcommit 500939d04a863f78b2af3c72c123deee3609469d[m
Merge: 7c36c41 7784ee4
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Feb 25 14:25:30 2019 -0800

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 7c36c412b386962585bcb51aa74c859d37b30ced[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Feb 25 14:24:37 2019 -0800

    Forward+backward compatibility fix around https://github.com/pytorch/pytorch/pull/15744

[33mcommit ed8236fa188c81426fba004ea198b05f0c1c4e38[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Feb 25 10:39:15 2019 -0800

    Fix for unscale usage in fp16_utils.FP16_Optimizer

[33mcommit d137b800f739b66e253636404885f2c4f0b9254b[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Feb 24 12:23:39 2019 -0800

    Stashing work

[33mcommit 7784ee4b66bc91e2acd2b468f01b45ee5fe5563f[m
Merge: 8e8dd35 bc76b01
Author: mcarilli <mcarilli@gmail.com>
Date:   Sun Feb 24 07:22:18 2019 -0800

    Merge pull request #167 from NVIDIA/syncbn_api_update
    
    update SyncBatchNorm API

[33mcommit 80a3f3cabd25cc1086cd9e5ffc7c8c3caab44e1a[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Feb 21 19:07:34 2019 -0800

    Allow multi-tensor unscale to handle FP16 output, so it can also be used for copy-scatter. Rename some options.

[33mcommit bc76b0187951badc2aabc500785f9e1c2540b1be[m[33m ([m[1;31morigin/syncbn_api_update[m[33m)[m
Author: Jie <jiej@nvidia.com>
Date:   Thu Feb 21 10:42:15 2019 -0800

    update SyncBatchNorm API for primitive implementation to support apex.parallel.convert_syncbn_model

[33mcommit 8e8dd35da6bf321efa58973fba9d1d765f080314[m
Merge: 187ed33 37cd5df
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Feb 20 14:52:16 2019 -0800

    Merge pull request #148 from NVIDIA/ekrimer_add_groupbn_test
    
    adding a test and example for sync (group) bn with group_size<world_size

[33mcommit 187ed33ebd06b19baa4e5ee852b2192ffa7881bd[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Feb 20 13:26:02 2019 -0800

    Update README.md

[33mcommit 5c78a50fa64c1165c4ecf9811479ee26b52d273f[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Feb 20 13:25:23 2019 -0800

    Update README.md

[33mcommit 8542db2811482d2e4397cbc7b08c91f4c1efb24d[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Feb 20 13:22:39 2019 -0800

    Update README.md

[33mcommit 0ff493d3103841ef0620952616f480f869fe4733[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Feb 20 11:32:29 2019 -0800

    Update README.md

[33mcommit 6212302e6f2ba7677609ec9e4db5fa5f39c40690[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Feb 20 10:49:28 2019 -0800

    Update README.md

[33mcommit 4cc1c1b468b794b4c70a1f31b8e96282b193f2c9[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Feb 18 21:46:15 2019 -0800

    Adding small python wrapper for multi_tensor_apply

[33mcommit 6763a8be34518e8c36005c5f7839b6539ef9dfe3[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Feb 18 21:39:18 2019 -0800

    Reworked multi tensor apply, added tests

[33mcommit 37cd5dfdf59a9f984863cc99ba7b9c3c4df8a618[m
Author: root <ekrimer@nvidia.com>
Date:   Sat Feb 16 00:41:31 2019 +0000

    moved process group creation into apex so it can be called by users

[33mcommit e49dca6efdb32b094fa8f7db89e5943aa64f13c8[m
Author: root <ekrimer@nvidia.com>
Date:   Sat Feb 16 00:08:54 2019 +0000

    fixing it to work properly in multi-node environment

[33mcommit 598fbc88a5be485db73ee0cbfba53925a8bc7c9e[m
Author: root <ekrimer@nvidia.com>
Date:   Sat Feb 16 00:05:42 2019 +0000

    adding test_groups.py to unit_test.sh

[33mcommit 889d17124c582e588b8b401e69152ef2670091d3[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Feb 12 23:15:06 2019 -0800

    New API tentatively works on resnet50, ready for stress testing.

[33mcommit fad78c1632357da2510249b1e604f8a094cc7200[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Feb 10 20:06:49 2019 -0800

    Stashing work

[33mcommit 69907b807886846b6cf7320b5b1e104b2d18c1ae[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Feb 10 15:40:50 2019 -0800

    Lazily trigger fused_norm for FusedAdam error on Windows

[33mcommit 8db3f95c8344952279ffb8882f55c2b60576da3c[m
Merge: 1f693b9 1b90385
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Feb 8 15:49:05 2019 -0800

    Merge branch 'master' into api_refactor

[33mcommit 1f693b927fc188d0aa6a12b630ef34a08cc1e96a[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Feb 8 15:48:30 2019 -0800

    stashing work

[33mcommit f5725555766599966a7ad09f566b09b2c8d4d658[m
Author: Evgeni Krimer <ekrimer@nvidia.com>
Date:   Fri Feb 8 14:52:01 2019 -0500

    printout message update

[33mcommit 18d412a627fbd498ce5bed17af51b1e3481e199a[m
Author: Evgeni Krimer <ekrimer@nvidia.com>
Date:   Fri Feb 8 14:47:13 2019 -0500

    a test and example for sync (group) bn with group_size<world_size

[33mcommit 1b903852aecd388e10f03e470fcb1993f1c871dd[m
Merge: b2f63c4 713e0fb
Author: ngimel <ngimelshein@nvidia.com>
Date:   Wed Feb 6 15:55:29 2019 -0800

    Merge pull request #144 from jma127/master
    
    Better FP16 support in pytorch fp16 utils.

[33mcommit b2f63c48408e1110bb1c7ad7bf6310141da30616[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Feb 6 13:08:15 2019 -0800

    Some documentation cleanup

[33mcommit 2cbca1a4ecabdc30f36aa7fbc91f9cbd3e4b7fd3[m
Merge: a9a3fe5 340e71a
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Feb 6 08:14:32 2019 -0800

    Merge branch 'master' into api_refactor

[33mcommit 340e71a4b1214d5b89d6b4c007bbbdb4059eeea7[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Feb 5 19:47:20 2019 -0800

    Tests for the fused downscale kernel

[33mcommit 8818ba9e283c541747639caf1dae6a8f344e8699[m
Merge: 9288ba5 a5bc76d
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Feb 5 19:45:11 2019 -0800

    Merge branch 'new_downscale_kernel'

[33mcommit a5bc76db88e27c1700dd15ce6a476cbc18d699cb[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Feb 5 19:41:38 2019 -0800

    Tests and resnet50 example work

[33mcommit 6e9159d8b420ee882c541e6d053735ebbff82612[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Feb 5 17:25:18 2019 -0800

    ready for testing

[33mcommit 713e0fb859c0299269eb038cf95e232ae2086444[m
Author: Jerry Ma <noreplyspamblackhole@gmail.com>
Date:   Fri Feb 1 13:38:47 2019 -0800

    Better FP16 support in pytorch fp16 utils.
    
    This commit adds an FP16Model class as a successor to network_to_half.
    
    The benefits of this class are:
    
    - Preservation of single-precision for BatchNorm layers. The models
      generated by network_to_half() convert BatchNorm moment tensors to
      half-precision, then back to single-precision, which hurts the
      accuracy of the moment estimators and occasionally results in NaNs.
    - Support for multi-argument nn.Modules (self-explanatory from code).

[33mcommit 9288ba5cbc0afe4c38d5b3585e0491d7e714633d[m
Merge: a11c45a 57ad184
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Feb 5 15:05:18 2019 -0800

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit a11c45a44702b3040e9b2a502fcaf9bdbb9f3dec[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Feb 5 15:04:55 2019 -0800

    Removing patching of loss.backward, which appears to cause memory leaks (reference cycles?) in some models

[33mcommit 337056c1f30b0ec01800bea35a5446d6a44ac41c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Feb 5 15:02:30 2019 -0800

    New downscale kernel is working but not perf tested

[33mcommit 57ad1840f985004f6afc74f26fe41634572c8a1d[m
Merge: 45537d3 475fca2
Author: mcarilli <mcarilli@gmail.com>
Date:   Tue Feb 5 11:41:45 2019 -0800

    Merge pull request #123 from donglixp/patch-1
    
    apex.optimizers.FP16_Optimizer: add state_dict() and load_state_dict()

[33mcommit 45537d343429539f1af9a0d578730f67b0c34ea1[m
Merge: d81ed26 03b0eeb
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Feb 4 21:16:12 2019 -0800

    Merge pull request #146 from NVIDIA/restore_fused_kernel
    
    Restore fused kernel

[33mcommit 03b0eeb805516e1d0330b52f31fe89cb93db6295[m[33m ([m[1;31morigin/restore_fused_kernel[m[33m)[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Feb 4 21:13:16 2019 -0800

    Only warn once in LossScaler constructor

[33mcommit a153c41a0f8401cd95f688d0c7b0c8d42c004ad7[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Feb 4 20:18:22 2019 -0800

    FP16 grad downscale (which shouldn't happen in user code) fallback + warning

[33mcommit d81ed26d187e18dc0879066f9fa58c15b87cbf1c[m
Merge: 48299b0 223a47e
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Feb 4 14:22:22 2019 -0800

    Merge pull request #143 from NVIDIA/sbn_no_affine
    
    allowing syncBN to run with affine = False

[33mcommit fd03f26a009ddf18eae12087b2d17bb5df908625[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Feb 3 19:02:42 2019 -0800

    Restoring fused inf/nan check + downscale kernel

[33mcommit 48299b0d3c0a56cd7d0b9fed6854d78e81df0cc7[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sat Feb 2 19:09:58 2019 -0800

    Lazy imports to reduce error spam

[33mcommit cc85a2e522055b11e2ce2515f078d3978b35d4b4[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Feb 1 11:37:01 2019 -0800

    async->non_blocking, module-specific logging

[33mcommit a9a3fe574954863a631158b5d7dd282baa945e08[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Feb 1 10:38:21 2019 -0800

    Stashing work

[33mcommit 859f528bca7b796476b71401336aa11210363350[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Feb 1 01:11:54 2019 +0000

    Making note of loss scaling in README

[33mcommit ae5982cbfca12413cbe4d628b6560e8e0f37247a[m
Merge: 43522e6 33512f9
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Feb 1 01:05:28 2019 +0000

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 43522e6345b94059d6f236ab66422fd7f173796c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Feb 1 01:05:24 2019 +0000

    Making static loss scale the default, and clipping master grads when running with --fp16

[33mcommit 33512f93fd203ab7819af37fd629822bcd7609be[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Jan 31 16:21:51 2019 -0800

    Update README.md

[33mcommit b83e38a6417bd41e4ce5edd81a5a9696abc9441d[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Jan 31 16:21:24 2019 -0800

    Update README.md

[33mcommit 223a47e9823adf5a16d0e133e110de5e48992384[m[33m ([m[1;31morigin/sbn_no_affine[m[33m)[m
Author: jiej <jiej@nvidia.com>
Date:   Thu Jan 31 16:03:38 2019 -0800

    allowing syncBN to run with affine = False

[33mcommit aed3086aadd3dee31d17b2327e117d9f509cf63a[m
Merge: b5465fe 9041a86
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jan 31 19:05:40 2019 +0000

    Merging in master

[33mcommit b5465fe65ddd79986b390b139fe744217d04b48f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jan 31 19:04:23 2019 +0000

    Removing spurious references to Penn Tree Bank results

[33mcommit 9041a868a1a253172d94b113a963375b9badd030[m
Merge: def8fb8 d9be3f9
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Jan 30 15:49:17 2019 -0800

    Merge pull request #142 from NVIDIA/update_word_language_model
    
    Update default dims in word_language_model to be multiples of 8 to enable Tensor Core use

[33mcommit d9be3f903f698e9ff1688daf8cd2305b93489be0[m[33m ([m[1;31morigin/update_word_language_model[m[33m)[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jan 30 23:47:52 2019 +0000

    clean README

[33mcommit e21946e03b8a51aad8539e358b8de83b80610430[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jan 30 23:33:43 2019 +0000

    Updated default sizes to be multiples of 8 to enable Tensor Core use.  Added performance guidelines to README.

[33mcommit def8fb859dbbed84ed877bbee7689656954a761a[m
Merge: 0b848f0 afc8d1b
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Jan 30 11:09:46 2019 -0800

    Merge pull request #100 from FDecaYed/deyuf/optimizer_unittests
    
    add unit tests for optimizers/fp16_optimizer

[33mcommit 0b848f0d93e83637519d39ab96447ee62c224213[m
Merge: 8b9ce24 fe365d5
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Jan 28 16:18:30 2019 -0800

    Merge pull request #137 from ngimel/bn_convert
    
    don't convert to float bn with affine=False

[33mcommit 8b9ce2446e61bccf323dfe63349ab4bb0a4eb7d3[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Jan 28 16:16:11 2019 -0800

    Update two_gpu_unit_test.py

[33mcommit 878ba51267c0af62a7cfe39924af31a742000fe8[m
Merge: 95fe7f6 d0624f4
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Jan 28 16:12:34 2019 -0800

    Merge pull request #138 from NVIDIA/sbn_test_cases
    
    [syncBN]

[33mcommit d0624f4fc62586b1fd9c2e50b3a4f74f20371fce[m[33m ([m[1;31morigin/sbn_test_cases[m[33m)[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Jan 28 16:12:09 2019 -0800

    Update two_gpu_unit_test.py

[33mcommit c8d7c9f152bc15026ed09fab5e49875748daf0ef[m
Author: jiej <jiej@nvidia.com>
Date:   Mon Jan 28 16:04:00 2019 -0800

    adding comment to explain single process gradient averaging

[33mcommit 63e47d29d967fa227ff4d5414c9e5f238305b6c7[m
Author: jiej <jiej@nvidia.com>
Date:   Mon Jan 28 13:48:47 2019 -0800

    [syncBN]
    test update to resolve
      https://github.com/NVIDIA/apex/issues/134#issue-403525480
    
    Using identical learning rate for both DDP with sync BN and single process BN.
    The previous configure leaves the impression that sync BN requires adjusting lr
    in the script, which is not true.

[33mcommit 95fe7f6a3f7cafeb0b0fbbd18f5b9e66fd8dfb07[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Jan 28 13:27:29 2019 -0800

    Update README.md

[33mcommit fe365d580fad3363b1b1b0a02f1178be8e387fce[m
Author: Natalia Gimelshein <ngimelshein@nvidia.com>
Date:   Mon Jan 28 18:29:50 2019 +0000

    don't convert to float bn with affine=False

[33mcommit c8bc3e62ad228dcbe91296bb829dd827b914e695[m
Merge: 06e11bd a88c09c
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri Jan 25 11:01:54 2019 -0800

    Merge pull request #132 from NVIDIA/testing_cache_fix
    
    Fix + tests for the eval->training caching issue

[33mcommit a88c09cfe1a86dcf05148f2459c32a802ceba9ea[m[33m ([m[1;31morigin/testing_cache_fix[m[33m)[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri Jan 25 10:51:37 2019 -0800

    Update explanation of is_grad_enabled() use

[33mcommit dfd40f9a72ea3c2f2f2658a967ee94daffc950e2[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jan 24 19:08:53 2019 -0800

    Adding tests, also, don't drop cache during eval.

[33mcommit 3b8e5c4a6e06b58992dcfc93c7022cb62f2bdabe[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jan 24 17:43:34 2019 -0800

    Removing some whitespace

[33mcommit a0ae9e9114789874125f127886cdcc294cc2b446[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jan 24 17:42:31 2019 -0800

    Removing some print statements

[33mcommit c7dcb0e13e9584802cb6692064aebe5e5b92b476[m
Merge: c619fe6 06e11bd
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jan 24 16:57:31 2019 -0800

    Merge branch 'master' into testing_cache_fix

[33mcommit c619fe6e385ba515b25c24c1088ad8ec10824eb1[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jan 24 16:55:26 2019 -0800

    Updating comments

[33mcommit 646fc0d09c835508a02fd0752ad2ca4dbb1c8a9c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jan 23 17:26:18 2019 -0800

    commenting out print statements

[33mcommit 56ea6d78d1139ea70c178e4c0565ae3d81c32cd8[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jan 23 17:22:44 2019 -0800

    saving for carl to review

[33mcommit 06e11bd38c41777a4490aa5908aedb7ff6680b3c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jan 23 10:43:08 2019 -0800

    Adding dummy _deactivate method

[33mcommit 79ad5a88e91434312b43b4a89d66226be5f2cc98[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri Jan 18 11:24:45 2019 -0800

    Update README.md

[33mcommit d9bce818d6f199901de744f4239f140c34c934b1[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri Jan 18 11:23:24 2019 -0800

    Update README.md

[33mcommit 4ef4fabfa5f6c900c197d3da6a64cec96b50b13d[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri Jan 18 11:22:54 2019 -0800

    Update Dockerfile

[33mcommit 7d05704c9dc761cd7818d996e0a4ea1e35f3d517[m
Merge: 438f6f9 38bada2
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Jan 17 16:26:18 2019 -0800

    Merge pull request #126 from NVIDIA/nhwc_sbn_patch_Pr
    
    patching grid reduction to be volta-safe

[33mcommit 38bada23526105434455f1d547e7a2e29b479d8a[m[33m ([m[1;31morigin/nhwc_sbn_patch_Pr[m[33m)[m
Author: Jie <jiej@nvidia.com>
Date:   Thu Jan 17 16:19:10 2019 -0800

    patching grid reduction to be volta-safe

[33mcommit 438f6f9fb2d81b9c449bb9a24f70d9a065b57c88[m
Merge: 3c7a0e4 a62b87e
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Jan 17 12:44:38 2019 -0800

    Merge pull request #125 from NVIDIA/nhwc_sbn_pr
    
    [sync BN nhwc]

[33mcommit a62b87ea7a0acfa08b672343ecfedac2dbf3fa38[m[33m ([m[1;31morigin/nhwc_sbn_pr[m[33m)[m
Author: Jie <jiej@nvidia.com>
Date:   Mon Jan 14 16:16:16 2019 -0800

    fixing utility function convert_syncbn_model to accept channel_last flag and properly set attribute for nested layers

[33mcommit 443fa76ec7eea8c3037494998437d5b528887462[m
Author: Jie <jiej@nvidia.com>
Date:   Mon Jan 14 16:01:57 2019 -0800

    [sync BN nhwc]
    
    Added kernel to support sync BN for channel last tensor

[33mcommit 475fca23e0aa8b8bf60a8479c2233cd10f7d44b9[m
Author: Li Dong <donglixp@gmail.com>
Date:   Fri Jan 11 19:45:08 2019 +0800

    apex.optimizers.FP16_Optimizer: add state_dict() and load_state_dict()
    
    Add state_dict() and load_state_dict() for apex.optimizers.FP16_Optimizer to store and recover optimizer states. These two methods are important to recover from a stored training checkpoint.

[33mcommit 3c7a0e4442c41bc5afa183fe4dd0672a729a3ec9[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jan 4 16:07:22 2019 -0800

    Adding amp imagenet example

[33mcommit 8421cfb44630ca285c7b1d5eb6715e1feabea406[m
Merge: 241dd6c fa719e8
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Jan 2 14:41:30 2019 -0800

    Merge pull request #113 from NVIDIA/sbn_issue
    
    [syncBN]

[33mcommit fa719e8b250543a76745623dc761ccf73bcc7f23[m[33m ([m[1;31morigin/sbn_issue[m[33m)[m
Author: Jie <jiej@nvidia.com>
Date:   Wed Jan 2 14:02:20 2019 -0800

    [syncBN]
    replacing new_group with torch.distributed.group.WORLD, avoids creating new
    group in every iteration.
    
    This should resolve the issue in Training gets stuck when using SyncBN #105

[33mcommit 241dd6c4fce8667b15aaf7196b39d801b084f9ae[m
Merge: a99e187 f429381
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri Dec 21 19:59:09 2018 -0800

    Merge pull request #110 from rxy1212/master
    
    Update __init__.py

[33mcommit f429381e855881d2dd2f227e0bf5bd8a4c010acf[m
Merge: a99e187 870c917
Author: rxy1212 <RyanDotStark@163.com>
Date:   Sat Dec 22 11:28:31 2018 +0800

    Merge pull request #1 from rxy1212/rxy1212-fix
    
    Update __init__.py

[33mcommit 870c917ae533f19d1ee70d473069574c19038782[m
Author: rxy1212 <RyanDotStark@163.com>
Date:   Sat Dec 22 11:27:31 2018 +0800

    Update __init__.py
    
    torch.distributed.new_group and torch.distributed.reduce_op are deprecated on pytorch 1.0.0, this fix can avoid some errors for now.

[33mcommit a99e18758090e859238e531702915aeeaaaed8f6[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Dec 17 19:14:32 2018 +0000

    Fix deprecation warnings for ReduceOp

[33mcommit 35891b280a76b49d55b5935784e71a0a10ce52f5[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Dec 17 10:30:15 2018 -0800

    Compatibility with new_group() API

[33mcommit 48343d94a068b6fd1d074ba3afe572b70181ffa8[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Dec 17 08:46:25 2018 -0800

    Adding retain_graph=True option

[33mcommit afc8d1b2b5d062372217ac8536762bd296ac24b9[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Fri Dec 14 16:14:28 2018 -0800

    add unit tests for optimizers/fp16_optimizer

[33mcommit 4212b3e943ea8d1b4c0c2749f4b7753de39a6d3a[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Dec 13 16:53:40 2018 -0800

    Attempt to fix 97 (not sure why it's happening to begin with)

[33mcommit 197bcc4893e9489025b63e23784c8c1ce02d779d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Dec 12 14:26:20 2018 -0800

    Warning instead of error if nvcc is not found

[33mcommit b56a20881e4b2023beb4ac392b551f6cef9e16a8[m
Merge: 3f9b5c9 b6188fc
Author: mcarilli <mcarilli@gmail.com>
Date:   Tue Dec 11 15:39:58 2018 -0800

    Merge pull request #92 from FDecaYed/deyuf/fused_adam
    
    WIP: improve fused adam

[33mcommit b6188fc4b939ace8cae76a20ece7fe32e315683d[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Tue Dec 11 12:39:09 2018 -0800

    address comment

[33mcommit d665ab90d30176296369ae3cbbecc7bf480b8d07[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Mon Dec 10 21:38:38 2018 -0800

    improve backward compatibility

[33mcommit f06feced6a161295c36f3795854f4e6aaf7ff305[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Mon Dec 10 21:22:35 2018 -0800

    better print

[33mcommit c8ca4bf4efa983fb2c2322df9495f4ca9f0c3b6e[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Mon Dec 10 20:46:39 2018 -0800

    remove temporarily added lr warmup
    This way optimizer will be more general, and warm up should be handled by user

[33mcommit 3f9b5c988c4b7674b1c83fee2f35fdc9b8dd1922[m
Merge: 920da6d 6d3c75e
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Dec 10 12:42:00 2018 -0800

    Merge pull request #94 from NVIDIA/sbn_util
    
    Adding process group in convert_syncbn_model

[33mcommit 6d3c75e5166dd890f70259bfaf5f4cd3f3179a97[m[33m ([m[1;31morigin/sbn_util[m[33m)[m
Author: Jie <jiej@nvidia.com>
Date:   Mon Dec 10 12:34:03 2018 -0800

    Adding process group in convert_syncbn_model

[33mcommit 4d9dcb5718dc30dc71c7a3e53d078364510d4cc3[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Fri Dec 7 13:35:26 2018 -0800

    address comments

[33mcommit be42aad5ec57440dae478616b1054046dcefa2df[m
Author: Deyu Fu <deyuf@nvidia.com>
Date:   Wed Dec 5 16:43:43 2018 -0800

    WIP: improve fused adam

[33mcommit 920da6dade4f63aa995f10a73d127ea7c5cb8195[m
Merge: 0273d7a 9ccebe5
Author: mcarilli <mcarilli@gmail.com>
Date:   Tue Dec 4 09:37:18 2018 -0800

    Merge pull request #89 from ptrblck/update_examples
    
    Update examples to PyTorch >=0.4.0

[33mcommit 9ccebe5b50823e638322e84c65a3526ea5d684b5[m
Author: ptrblck <piotr.bialecki@hotmail.de>
Date:   Tue Dec 4 14:59:39 2018 +0100

    call .float() on GPU, remove unnecessary push to GPU

[33mcommit 0273d7ad872cf5eb7f4905ff64f7c6feb1432c7a[m
Merge: 5dad4c2 ee67e56
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Dec 3 15:51:43 2018 -0800

    [syncBN] (#77)
    
    adjusted kernel config for better perf.
    removed divergence in welford warp reduction.

[33mcommit 5dad4c21627f473885438f312a3c817e26e763b5[m
Author: jjsjann123 <jiej@nvidia.com>
Date:   Mon Dec 3 15:49:12 2018 -0800

    [syncBN] (#90)
    
    supporting user specified process group

[33mcommit 28bdc04eec8304c4985bb6625828f3c8fdaf321e[m
Author: ptrblck <piotr.bialecki@hotmail.de>
Date:   Sun Dec 2 01:39:17 2018 +0100

    update examples to PyTorch >=0.4.0

[33mcommit bc62f325384a62663901f48a09001ca9477646e9[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Nov 30 13:14:44 2018 -0800

    Syncing main.py a bit more

[33mcommit 2a8022ca5b0b011191fcd394518344217b762c46[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Nov 30 13:11:19 2018 -0800

    Adding deterministic option to main_fp16_optimizer.py

[33mcommit b436213ec35fda21fd6260b2ce8b5d212c5072fc[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Nov 28 14:35:59 2018 -0800

    minor latex touchup

[33mcommit 98b76a86da473b3a58d047eed4a41c97dbb72492[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Nov 28 11:38:37 2018 -0800

    Shortening import path for layernorm

[33mcommit 67ad3065b828dcb2f25176789781d2d2758e0171[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Nov 28 11:27:26 2018 -0800

    Adding layernorm docs

[33mcommit 64f3d3624f077fe4d9d2c8cbeef3feee69e1070f[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Nov 14 12:11:45 2018 -0800

    Distributed backend compatibility update

[33mcommit 2b8277e5f316592cfa7d968230d4000466bdca60[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sat Nov 10 15:54:03 2018 -0800

    Updating example instructions to use batch size 224 for safety

[33mcommit ee67e56adbc0d2f4f6471f5b53b4ba7775b0f2eb[m[33m ([m[1;31morigin/syncbn_pr[m[33m)[m
Author: Jie <jiej@nvidia.com>
Date:   Wed Oct 24 17:02:39 2018 -0700

    [syncBN]
    adjusted kernel config for better perf.
    removed divergence in welford warp reduction.

[33mcommit 8bd382fadd42f77922fbc3dfacc3e4cc2f89ece5[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Nov 1 15:20:31 2018 -0700

    Minor docstring update

[33mcommit da515dca822fa1257c853594a092888c15d8a05b[m
Author: schetlur <sharanyan.chetlur@gmail.com>
Date:   Thu Nov 1 15:02:54 2018 -0700

    Update adamopt docs (#73)
    
    * Adding some missing fields to adamopt documentation.
    
    * Adding some clarification to documentation.

[33mcommit 97ab5ad3056f3331e6a771e0c40e51fc9d4f44eb[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Nov 1 11:44:03 2018 -0700

    Docstring updates

[33mcommit efc561ba8ca09520f589afa44a21d2b32b022c4c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Nov 1 11:27:49 2018 -0700

    Adding switch to control averaging of gradients.

[33mcommit 1b9b65ca25be8b9ce5ed44a224d6658a0a7394e1[m
Author: Thor Johnsen <tjohnsen@nvidia.com>
Date:   Wed Oct 31 11:22:06 2018 -0500

    [WIP] Fused layer norm cuda (#69)
    
    * Pre-release of fused layer norm apex extension
    
    * Remove half and __half2 specializations
    
    * Code changes from review

[33mcommit 45f030dbeb412bca0ca5f84754f0c81e8512d209[m
Author: ngimel <ngimelshein@nvidia.com>
Date:   Tue Oct 30 16:48:24 2018 -0700

    Remove arch from adam compile options

[33mcommit daea4188992e56c4fa13220d4a243b6b23de60bf[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Tue Oct 30 15:47:44 2018 -0700

    Adding some missing fields to adamopt documentation. (#70)

[33mcommit d594826cceb79875663b798b5cc38daceaf8ce8b[m
Author: ngimel <ngimelshein@nvidia.com>
Date:   Tue Oct 30 14:29:06 2018 -0700

    Adam tests (#67)
    
    * Add unittest for FusedAdam.
    
    * Fix some bugs.
    
    * set seed for adam test

[33mcommit a01a732676cd6aa7206e9824987f3ef92e7c563d[m
Merge: 8124fba ef3a002
Author: ngimel <ngimelshein@nvidia.com>
Date:   Tue Oct 30 13:33:06 2018 -0700

    Merge pull request #68 from ngimel/includes
    
    update includes

[33mcommit ef3a00259cc541bc7ee726b2d348c57f91788686[m
Author: Natalia Gimelshein <ngimelshein@nvidia.com>
Date:   Tue Oct 30 11:09:39 2018 -0700

    update includes

[33mcommit 8124fba2c572efc3a1f267b438d102eca17f9ce4[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Oct 30 01:59:45 2018 +0000

    Updating documentation for merged utilities

[33mcommit 1fa1a0737dbec3ddbe2181f2236a808827a5add1[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Oct 30 00:39:31 2018 +0000

    Warning message for FusedAdam import if unavailable

[33mcommit e0bc5d62a15990bad1f31ea97994f84d2d3029c9[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Oct 29 16:40:51 2018 -0700

    Merging in fused adam optimizer, additional DDP features tested in 18.10 (#60)
    
    * test passes
    
    * notes
    
    * Using C++-side flatten and unflatten functions
    
    * Adding csrc
    
    * Persistent synchronization event so it doesn't need to be created and destroyed each time
    
    * Interop with parameter flattening in SSD
    
    * Added deterministic option to imagenet main.py
    
    * Adding options to split gradient averaging and allreduce in pure fp32
    
    * Fixing allreduce_maybe_retain call
    
    * Fixing allreduce_fallback
    
    * Also sync active_i_buckets from rank 0
    
    * Making retain_allreduce_buffers compatible with/orthogonal to delay_allreduce=True|False
    
    * Correcting syntax error, now all seems to work with SSD
    
    * Optional cpp extension build
    
    * Add mixed precision adam optimizer (#59)
    
    * Add FusedAdam Optimizer to Apex that places all the math into a cuda kernel.
    
    * Added fixes to fused_adam to get it to work with network.
    
    * wip work on python interface for adam with options
    
    * fix dispatch for halfs, add python options to handle optional half gradients and params
    
    * cleanup, get rid of grid-stride loop

[33mcommit 81eef1ef60f419babf66d52b54e2118390d4da31[m
Author: jjsjann123 <jiej@nvidia.com>
Date:   Tue Oct 23 11:19:45 2018 -0700

    [syncBN] (#48)
    
    * [syncBN]
      added syncBN in native pure python apex
      added fused cuda kernels used for sync BN. Using welford for mean/var
        optional installation using 'python setup.py install --cuda_ext'
      added unit test with side to side comparison between apex sync BN with
        PyTorch BN. Notice that for pytorch BN implementation, because of
        numerical issue for mean/var, the output will be slightly off.
    
    * [syncBN PR]
      added fp16 support
      addressing review comments on:
        1. updating last pow 2
        2. look for import error when importing syncBN kernel
    
    * [syncBN PR]
      added convert function to insert SyncBatchNorm
      refactored some kernel code
    
    * fixing type issue (fp16/fp32/fp64)
    added Kahan summation
    editing unit test to use pytorch primitive ops with double, passing reasonable tests now
    
    * updating tensor creation calls
    
    * fixing the all_reduce contiguous tensor
    
    * transposed all reduce results
    
    * [syncBN]
    support fp16 input & fp32 layer for apex fp16
    partially fixing launch configs
    enabling imagenet example to run with --sync_bn
    
    * [syncBN PR]
    Documentation added
    
    * adjusting README
    
    * adjusting again
    
    * added some doc to imagenet example
    
    * [syncBN]
      warp-level reduction
      bug fix: warp reduction logic updated. check for dummy element to avoid nan.
      improved launch config for better reduction kernels. Further improvements
    would be to increase grid size.
    
    * [syncBN]
      fixing undefined behavior in __shfl_down_sync from divergent threads in warp
    reduction.
      changing at::native::empty to at::empty (upstream comments)

[33mcommit e12c1ec300c6b7369e17ba733996eacefae462a9[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Oct 10 13:02:05 2018 -0700

    Docstring updates

[33mcommit 8add2b078699583e48a395ce00716a7454a5bc4d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Oct 10 08:59:12 2018 -0700

    Docstring updates

[33mcommit cd78831706aa6a2b75a65c54c1f965dcb428fef4[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Oct 8 15:07:04 2018 -0700

    Update README.md

[33mcommit fd9b02c0504599727623fb3f3c1271bc9098428c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Oct 8 10:56:58 2018 -0700

    Moving gradient division back to after the allreduce.  Empirically, it appears underflow is more of a danger than overflow.

[33mcommit 9eab1ac33903017656c9751fc3121b604e1287dd[m
Merge: 559141e 2361a64
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Oct 7 13:53:03 2018 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 2361a64688be8ddf6e325e2b59d88707a34887ec[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Oct 7 09:23:14 2018 -0700

    Updating imagenet FP16_Optimizer example for new syntax

[33mcommit e4c97f32b44fdc32a1649c6779da9a921fded6ca[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Oct 5 20:42:09 2018 +0000

    Adding set_grads_to_None option to fp16_optimizer

[33mcommit e4af2d909eabfe4c2373b1a93b6e2262087adcdb[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Oct 3 15:28:23 2018 -0700

    Move gradient division to before the allreduce
    
    This is consistent with upstream, and safer against overflow.

[33mcommit 2f204bcaddd3ac2b5bf705a94a7aa207f0fda049[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Sat Sep 29 14:48:27 2018 -0700

    fix error message

[33mcommit 89fa152bc16e83f646e49ece389792f73b02601b[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri Sep 28 20:52:38 2018 -0700

    Move other logic after forward to take advantage of GPU skew

[33mcommit 9d731777ef1316e43b37bf30dfcae3d41d7ebefd[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sat Sep 29 01:03:37 2018 +0000

    Clean up race condition test, need to figure out a clean way to create distributed unit tests

[33mcommit fa183ee8eabe115215fd5a18e6b6cea836261ab4[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri Sep 28 17:17:12 2018 -0700

    Efficient bucketing (#49)
    
    * beautiful
    
    * IT'S WORKING
    
    * Hopefully fix race condition for fallback hook
    
    * Updating test
    
    * shared_param -> delayed_allreduce
    
    * Adding a safety check
    
    * One more check
    
    * syntax...

[33mcommit 53e1b61a1e2498e66e4af9ff19e0bc55955b24b0[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Tue Sep 18 18:15:02 2018 -0700

    Fix param freezing (#47)
    
    * Fix appears to work in Tomasz's example.
    
    * Somehow shared_param got de-enabled again?

[33mcommit ed47ebff3cda136972c7d1c63b807dda0938e2b0[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Sep 18 13:39:21 2018 -0700

    Forward compatibility fixes for distributed backend, thanks to @Ssnl

[33mcommit 0ec8addb201c22971e3608a948dbc8576c59e6b8[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Mon Sep 17 19:42:04 2018 -0400

    Remove some fp16 examples that don't converge (#45)
    
    * Remove some fp16 examples that don't converge
    
    Default static loss scale of 1.0 (default value) for resnet50 doesn't converge. Either remove example or put static loss scale 128 on it, which is known to converge well.
    
    * Update README.md

[33mcommit 48f105d92ad8e73b58eb49a483a4ae4ea72fa00c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Sep 14 15:37:28 2018 -0700

    Only save and load master params if training with FP16

[33mcommit 327b24469236e467ce3a91fac9fea6a3540af56d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Sep 13 17:35:33 2018 -0700

    Fixing imagenet main.py and main_reducer.py to save and load master params

[33mcommit b7025fc9eaa8180783af48a9c527979c3a594a97[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Sep 13 13:35:36 2018 -0700

    Skeleton for modular tests

[33mcommit 3e1a1c097b25c9b32f21fb157ac2621ef5f5a65b[m
Author: Carl Case <cbcase@gmail.com>
Date:   Tue Sep 11 08:36:35 2018 -0700

    amp support for Aten RNNs (#41)
    
    * WIP: update to support new RNN backend code
    
    * small refactor
    
    * add test for rnn w/ packed sequences

[33mcommit 1579b9e3db87517e75933e08707b15e074717f2f[m
Merge: 75a865e 5f5dfa4
Author: Carl Case <cbcase@gmail.com>
Date:   Mon Sep 10 14:59:33 2018 -0700

    Merge pull request #40 from NVIDIA/amp_tests
    
    amp unit tests

[33mcommit 5f5dfa42e682a1d834e8cc9bf70ccfc1b6a648a3[m
Author: Carl Case <carlc@nvidia.com>
Date:   Mon Sep 10 14:18:54 2018 -0700

    add rnn tests

[33mcommit 39928327b000cbbc99abeb170fa46b76f85c1d58[m
Author: Carl Case <carlc@nvidia.com>
Date:   Mon Sep 10 13:10:20 2018 -0700

    tests on banned methods

[33mcommit 22920fe091a7bd03eb92d03606d2572bdf618894[m
Author: Carl Case <carlc@nvidia.com>
Date:   Mon Sep 10 11:24:45 2018 -0700

    add more promotion testing

[33mcommit 75a865e3b1668ae47a1dc86509d7fe0c12dab1d7[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Sep 7 09:06:01 2018 -0700

    Adding verbose option to FP16_Optimizer

[33mcommit cb6d8f1acc107e987f0641e6ec7c4199d39867bb[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Sep 6 20:12:09 2018 +0000

    Enabling single-process fallback for examples/imagenet/main_reducer.py

[33mcommit a2801d91f9bc2e796b16eb2095e5d450c9f4422e[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Sep 6 13:03:14 2018 -0700

    Revising LR scaling to account for any choice of num processes, batch size per process

[33mcommit 01e29c97358e3b0dd52964cbc9bf262db70ca3a3[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Sep 5 13:58:23 2018 -0700

    minor fix

[33mcommit ed14f39c230f41e28068e72d5fa61d3be2fb131c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Sep 5 10:56:11 2018 -0700

    Fixing needs_refresh logic to allow multiple forwards between each backward

[33mcommit 586c507e9720fda5f612844bf82e99d7c0afa9cc[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Aug 30 16:32:29 2018 -0700

    Update README.md

[33mcommit 5a39c5e390c3e27e7ae3eef08b7a4b517db0f0ac[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Aug 30 09:42:52 2018 -0700

    Update distributed.py

[33mcommit 559141e83953d92845c3a644c1d0072e5e5508ca[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Aug 28 16:05:15 2018 -0700

    Reformatting

[33mcommit 034b8f029c77f4ad4781b7f0a7f7ff31cb0e258d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Aug 28 14:13:49 2018 -0700

    Updating imagenet README

[33mcommit 37a4b2219f8f344d3158f3c5ce52ca7fa49fed75[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Aug 28 14:07:41 2018 -0700

    Cleaning up git weirdness + updating docs for Reducer

[33mcommit f09fb4f411533ec229ff3bf9ce8be72f0fff38e7[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Tue Aug 28 15:35:16 2018 -0400

    Reducer (#38)
    
    * Add reducer class in parallel/distributed.
    
    * Separate DDP and Reducer examples.
    
    * Don't confuse DDP and reducer in example.

[33mcommit 73b62ddef98f3cc20405e507ce316da5ec82005b[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Tue Aug 28 14:34:53 2018 -0400

    Add reducer class in parallel/distributed. (#37)

[33mcommit dc41c5ce17688283aa103ac753b247744d64ed96[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Aug 27 18:32:46 2018 -0700

    Adjusting learning rate for batch size

[33mcommit 81c788f037e75e67612468fce06c5c26130b1c60[m
Author: Carl Case <carlc@nvidia.com>
Date:   Mon Aug 27 17:18:00 2018 -0700

    WIP: promotion tests

[33mcommit 2e69d93385315a2767f6f47360ce1f85e6f93d7a[m
Author: Carl Case <carlc@nvidia.com>
Date:   Mon Aug 27 15:44:02 2018 -0700

    add first basic amp unit tests

[33mcommit ebaa5a150dfae437e61ab0864e2f9e0be68d6c6b[m
Author: Carl Case <carlc@nvidia.com>
Date:   Mon Aug 27 15:42:42 2018 -0700

    experimental: ability to deactivate amp with handle

[33mcommit 437bcf2243158154b431ac8a538371971f3f33c2[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed Aug 22 10:36:14 2018 -0700

    Update README.md

[33mcommit 12dce88d30cde716cc5ad1ea1af3e618a7c540e7[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Aug 19 18:20:02 2018 -0700

    Disabling import of experimental/planned files

[33mcommit 59bf7d139e20fb4fa54b09c6592a2ff862f3ac7f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Aug 20 00:55:35 2018 +0000

    minor cleanup

[33mcommit 179712024ec757bdc7fa6f91d574a7592015e394[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sat Aug 18 19:47:51 2018 -0700

    updating FP16_Optimizer example as well

[33mcommit eae8b98964616757ef73b21b67384dc420ae4cc2[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sat Aug 18 19:18:59 2018 -0700

    Adjusting learning rate schedule for 76% accuracy

[33mcommit 39c9be8577eaaaa61a4097b62eae399a2e549f57[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Aug 16 10:26:29 2018 -0700

    Syncing imagenet examples to use distributed validation

[33mcommit 7d0aef9f5a5de1c7e1bb30778126685ec8b469b8[m
Merge: 2af29c1 ae3be17
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Aug 16 10:04:40 2018 -0700

    Merge branch 'dist_valid'

[33mcommit 2af29c1977031d72317f7883d93d8ab22fe01bf8[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Aug 16 10:00:24 2018 -0700

    Removing orphaned /distributed/run_distributed.sh

[33mcommit 1d45fada24436e3bfde126d865505583c30a6512[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Aug 14 15:55:46 2018 -0700

    Fix to enable freezing params

[33mcommit ae3be17a195af2b88e75406bad56ee44430c9a76[m[33m ([m[1;31morigin/dist_valid[m[33m)[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Tue Aug 7 18:20:28 2018 +0000

    Dist validation imagenet main.py

[33mcommit 2063287b42f3ca1fcb7957b7f9c962405a75d0dc[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jul 25 18:41:02 2018 +0000

    Using temporary Python-side inf+nan check in amp/scaler.py

[33mcommit ae921de24fe7c6851c26d1127fcbd2c4fe4ebd9d[m[33m ([m[1;31morigin/python_only[m[33m)[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Jul 24 00:55:56 2018 +0000

    Fixing FP16_Optimizer handling of LBFGS

[33mcommit d695b68b36890e0c6f38de0897d3334bd0db4586[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Jul 23 15:30:53 2018 -0700

    Switch to simple Python-only install, in preparation for upstreaming C++ backend.

[33mcommit aa81713249bb17d715c247adeadc229a37adeefa[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jul 19 17:35:40 2018 -0700

    Ported examples to use torch.distributed.launch

[33mcommit 12b49b98aff21a067448ab8d96f8250614ca1e84[m
Author: ngimel <ngimelshein@nvidia.com>
Date:   Wed Jul 18 10:34:06 2018 -0700

    delete __getstate__ arg

[33mcommit 77ee4bcd73155de2981fe54cfcfefa216c12f60a[m
Merge: 5c6144e f1f97f9
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Wed Jul 18 13:01:57 2018 -0400

    Merge pull request #30 from NVIDIA/checkpoint_fix
    
    Handle set/get state for DDP, remove stream which cant be pickled.

[33mcommit f1f97f9f6d7d33fb842436daae9b78f6774f92bc[m[33m ([m[1;31morigin/checkpoint_fix[m[33m)[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Wed Jul 18 16:50:15 2018 +0000

    Handle set/get state for DDP, remove stream which cant be pickled.

[33mcommit 5c6144e63593ca7cfd4c47469eb2a55de0e6891c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Jul 9 15:22:06 2018 -0700

    FP16_Optimizer now preserves param order and casts per-param state tensors to FP32

[33mcommit 4a8cf7ad3e5c64bd38395e30ebc59e89155ab488[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Jul 9 09:14:02 2018 -0700

    Fix single-GPU fallback for examples/distributed

[33mcommit 31931985a7a9c9ddc42dc0d577bcb155c219b617[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Jul 5 11:23:01 2018 -0700

    Update README with Windows build instructions

[33mcommit 738f76078409451feee5789797d6142120c976a2[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Jul 5 10:23:52 2018 -0700

    Update README.md

[33mcommit 247349f1de28ef94e863aa2b28bbb7adcd52c49b[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Jul 5 09:32:45 2018 -0700

    Update setup.py for #23

[33mcommit d74fda260c403f775817470d87f810f816f3d615[m
Author: brett koonce <koonce@hello.com>
Date:   Tue Jul 3 22:12:34 2018 -0500

    docs: minor spelling tweaks (#25)

[33mcommit a21a945c841a9596de5c63e692692fd4646c4883[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Tue Jul 3 15:17:59 2018 -0700

    Update fp16_optimizer.py

[33mcommit 88effd5da483a1ce5ce98b6800943dcac2057f2d[m
Author: Raul Puri <raulpuric@berkeley.edu>
Date:   Tue Jul 3 13:17:49 2018 -0700

    LARC clipping+documentation (#6)
    
    * Proper implementation of LARC clipping
     * Documentation of LARC class
     * Modification of FP16_Optimizer to absorb optimizer instance that's being wrapped instead of creating new optimizer instance of same class.

[33mcommit 34582381e289287e6d09490b69c0840718729f2e[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 29 16:05:18 2018 -0700

    Syncing imagenet examples

[33mcommit d6b2e7d3b03cb73cc81070102452beca36774a48[m
Merge: 5e54253 6f0748d
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 29 15:18:04 2018 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit 5e54253fa6b1e4180b55565212b10fc2dae50d1f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 29 15:18:01 2018 -0700

    Updating examples

[33mcommit 6f0748d68bdaf1a15b2101e751a6c272cd1adcb9[m
Merge: 1d2094a cf45c54
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Fri Jun 29 16:22:29 2018 -0400

    Merge pull request #22 from romerojosh/master
    
    Fixes to validation in imagenet example scripts.

[33mcommit cf45c54cdec3ae37b9d5ffcfb02e425280d66d05[m
Author: Josh Romero <romero.josh@gmail.com>
Date:   Fri Jun 29 13:07:08 2018 -0700

    Fixes to validation in imagenet example scripts. Precision and loss reporting modified to be consistent with train.

[33mcommit 1d2094a144da2f065ec7b2556601abc243cd9cb6[m
Merge: 80479ee 21c229e
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Wed Jun 27 23:22:21 2018 -0400

    Merge pull request #21 from romerojosh/master
    
    Fix to imagenet main.py data normalization.

[33mcommit 21c229e0c1a08e98140faf9166af7233af1d508b[m
Author: Josh Romero <romero.josh@gmail.com>
Date:   Wed Jun 27 17:49:27 2018 -0700

    Fix to imagenet main.py data normalization.

[33mcommit 80479eed9dd5509df9cea845801b11bf8a21e5e2[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Tue Jun 26 10:02:42 2018 -0700

    More stringent check for parameter changes to trigger refresh of distributed (#20)
    
    * More stringent check for distributed refresh

[33mcommit a7319ceeeed7780661adeb023e52e0df92780ba4[m
Merge: 3fa300b 68c96f4
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Tue Jun 26 10:28:02 2018 -0400

    Merge pull request #18 from rotabulo/fix_ddp
    
    Fixed deadlock issue

[33mcommit 68c96f4cd6be739e0e91acff384861eb3c94b364[m
Author: Samuel <samuel@mapillary.com>
Date:   Tue Jun 26 15:30:09 2018 +0200

    Fixed deadlock issue

[33mcommit 3fa300b91d1dc440ef39cb0621935ea5dedaec58[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Jun 24 15:43:11 2018 -0700

    cosmetic

[33mcommit 2e7d799f9cc5b8544c9ef07330c4cd7eacd894ee[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sun Jun 24 15:36:39 2018 -0700

    Response to issue #16:  more comprehensive instructions in examples/docker

[33mcommit d52edb9eed21be5fd0d3f739f5d6870a27faada9[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 22 17:47:21 2018 -0700

    FP16_Optimizer + dynamic loss scaling now works with optimizers that require closures, e.g. LBFGS

[33mcommit 6e39bee32bc569d427099c3fc29356a8ad06b1ce[m
Merge: bfa3e0e 251cdda
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 22 14:10:56 2018 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit bfa3e0eed76abf098af32854c9dca443e899f736[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 22 14:10:47 2018 -0700

    Adding simple example links for distributed

[33mcommit 251cddaf33fbe2e7ea6e558ee5be5aa48d0bfb03[m
Merge: 06d035e bddbbdc
Author: mcarilli <mcarilli@gmail.com>
Date:   Fri Jun 22 10:23:38 2018 -0700

    Merge pull request #15 from syed-ahmed/master
    
    Adding changes due to https://github.com/pytorch/pytorch/pull/8660

[33mcommit bddbbdcbcb7dc830a989befcc54195e5142cd68d[m
Author: Syed Tousif Ahmed <syeahmed@nvidia.com>
Date:   Fri Jun 22 10:11:13 2018 -0700

    Reverting changes.
    
    Co-authored-by: Michael Carilli <mcarilli@gmail.com>

[33mcommit 06d035e3ae8b2900e73de566c760767893c4c609[m
Merge: 523be48 2e32e24
Author: Carl Case <cbcase@gmail.com>
Date:   Thu Jun 21 08:28:33 2018 -0700

    Merge pull request #13 from cclauss/patch-1
    
    Type in variable name: hidden --> hiddens (w/ trailing 's')

[33mcommit 2e32e245bad81c7e25a8727768aeda54109b3bc3[m
Author: cclauss <cclauss@bluewin.ch>
Date:   Thu Jun 21 03:04:50 2018 +0200

    Type in variable name: hidden --> hiddens (w/ trailing 's')
    
    __hidden__ in an undefined name in this context but __hiddens__ is used on the lines above.
    
    flake8 testing of https://github.com/NVIDIA/apex on Python 3.6.3
    
    $ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
    ```
    ./setup.py:50:23: F821 undefined name 'ctypes'
            cudart_path = ctypes.util.find_library('cudart')
                          ^
    ./apex/amp/wrap.py:194:37: F821 undefined name 'hidden'
                elif utils.is_fp_tensor(hidden):
                                        ^
    ./apex/amp/wrap.py:195:41: F821 undefined name 'hidden'
                    new_args.append(cast_fn(hidden))
                                            ^
    ./apex/amp/wrap.py:198:33: F821 undefined name 'hidden'
                    new_args.append(hidden)
                                    ^
    4     F821 undefined name 'hidden'
    4
    ```

[33mcommit 523be4830675d574cf38a7c4e8294a08689cf5b0[m
Merge: adaa913 5f2c649
Author: ngimel <ngimelshein@nvidia.com>
Date:   Wed Jun 20 17:40:44 2018 -0700

    Merge pull request #12 from cclauss/patch-1
    
    import ctypes for line 51 of setup.py

[33mcommit 5f2c649a85ef1c1a258b231da6a100d44c77952f[m
Author: cclauss <cclauss@bluewin.ch>
Date:   Thu Jun 21 02:37:58 2018 +0200

    import ctypes for line 51 of setup.py

[33mcommit adaa9137c17b495f23fc34447c02ba839d037cff[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Jun 19 17:54:44 2018 -0700

    Readme updates + version in Sphinx

[33mcommit 0a092aaf715129c8a288007d470e2a984a87e4c4[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Jun 18 16:28:37 2018 -0700

    Updating clip_master_grads for forward compatibility

[33mcommit 7eba6bfb5bd0e6569547b62ae44c3d0d7dbd028f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Jun 18 15:45:54 2018 -0700

    Added checkpointing example.

[33mcommit ed98834d2f42817259fd95656696b5dca2fd955c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 15 18:51:02 2018 -0700

    Adding sphinx rst for amp

[33mcommit c41d9f2bea31f5e926f75e0c26e8e69e0994689f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 15 18:47:26 2018 -0700

    README wiring in a reasonable state, Sphinx docstrings updated

[33mcommit 5f8c31835a0871fadaf99fc4f49d56f774890dfa[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 15 16:46:59 2018 -0700

    More docstring + README updates

[33mcommit 82d7a3bf0555004c653fac877410236dcbaa2b7a[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 15 11:45:54 2018 -0700

    Updating READMEs and examples

[33mcommit 2c175a5d1436b40105cd73898cbcbed74f053e07[m
Merge: e215dd4 38cb0ce
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 14 17:15:58 2018 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit e215dd41ad314bdd07e02736c53beebdf47ccd33[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 14 17:13:18 2018 -0700

    Updating READMEs

[33mcommit 38cb0ce464c85415e2b459b329444811a456f996[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Jun 14 16:51:29 2018 -0700

    Update README.md

[33mcommit 18afea3b3427e2386574bab66fc8e745f4a600fe[m
Merge: f5790a1 671f976
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 14 16:50:25 2018 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit f5790a1e67e348dd951a5684edb93e436d5cbf9f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 14 16:49:57 2018 -0700

    Adding closure and minimal examples

[33mcommit 671f976e9a3193517ef52486ae9d1889b4107372[m
Author: mcarilli <mcarilli@gmail.com>
Date:   Thu Jun 14 15:35:30 2018 -0700

    Update README.md

[33mcommit 716719931f298d851c545d98a975bd727c62571e[m
Merge: b78b5ea 43d1ae0
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 14 15:14:55 2018 -0700

    Merge branch 'master' of https://github.com/NVIDIA/apex

[33mcommit b78b5ea501acedde19caf6ed809c09d3e58726eb[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 14 15:13:20 2018 -0700

    Editing Dockerfile

[33mcommit 98fa5a3b4e5a703a6c0676c2caf1880ddde821d8[m[33m ([m[1;31morigin/preview_readme[m[33m)[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 14 14:59:02 2018 -0700

    Pulling in old logic to manually look for CUDA_HOME in Pytorch <= 0.4 to allow cross-compilation

[33mcommit 0f703d13ce8303182cb86ac727bb804dffb4ecc8[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 14 13:55:40 2018 -0700

    Added Dockerfile example, more readme updates

[33mcommit 942174bf15dd73881d8b7396922b4c90200841b3[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 14 09:57:27 2018 -0700

    Changing loss_scale to static_loss_scale in imagenet/main.py to be explicit

[33mcommit 2a2341c7adf35c6482d23f8c7baa4add6c6917d4[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 14 09:26:37 2018 -0700

    Rewiring READMEs

[33mcommit 43d1ae08340edf1f27832cea863cb612c6bbf8f1[m
Merge: 227a9a2 32fbbe4
Author: Carl Case <cbcase@gmail.com>
Date:   Tue Jun 12 21:54:38 2018 -0700

    Merge pull request #11 from NVIDIA/amp_lstm_backward
    
    Handle the use of .sum() in fused LSTM/GRU backward

[33mcommit 32fbbe488a52214a613bee2863c041a3c96f2c43[m
Author: Carl Case <carlc@nvidia.com>
Date:   Tue Jun 12 21:53:13 2018 -0700

    Handle the use of .sum() in fused LSTM/GRU backward

[33mcommit 227a9a2ddffd4e29c90a275f43c5181a5b6cb5a3[m
Merge: 378ce1e f32a0a6
Author: Carl Case <cbcase@gmail.com>
Date:   Tue Jun 12 16:05:09 2018 -0700

    Merge pull request #10 from NVIDIA/amp_cpu_fix
    
    amp shouldn't convert CPU tensors

[33mcommit f32a0a63e931282e0703c4cdb71366eecceca21d[m
Author: Carl Case <carlc@nvidia.com>
Date:   Tue Jun 12 16:03:13 2018 -0700

    Don't touch gpu tensors in functional API

[33mcommit 378ce1e144b0a09574b2f3e02ec4052ab2b68030[m
Merge: fb075b8 06ee98c
Author: mcarilli <mcarilli@gmail.com>
Date:   Mon Jun 11 10:52:56 2018 -0700

    Merge pull request #9 from NVIDIA/imagenet_fix
    
    DDP fix, imagenet speedup

[33mcommit 06ee98c2779010330bc4d7108ac104d9f2df50d6[m[33m ([m[1;31morigin/imagenet_fix[m[33m)[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Fri Jun 8 21:14:04 2018 +0000

    [Imagenet example] Switch validation to same I/O pipeline so validation produces correct result.

[33mcommit 421c9e66e271eb85732927cc06d913b8d8f80b14[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Fri Jun 8 21:13:03 2018 +0000

    Switched to non-bucketed comm

[33mcommit 137d822b2dffc467b45f4c3159f611098fe892be[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Fri Jun 8 21:11:57 2018 +0000

    Better shortcut for shared_param = True

[33mcommit fb075b86c89428b9e64e5e86aceb73c47fa45d93[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Jun 8 10:36:23 2018 -0700

    Compilation succeeds on 0.4, 18.04-6 containers, and current upstream master

[33mcommit bf855389ff9126a0954f9da57152000df21b380b[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Jun 7 16:26:25 2018 -0700

    Reverting setup.py to rely on cpp_extension's native CUDA_HOME, because upstream fixed cross-compilation

[33mcommit 70b337705397eefdb66fe812403e3d956252e909[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jun 6 17:02:22 2018 -0700

    Updating setup.py to enable cross-compilation

[33mcommit af431d283192b15556ac44689dddf979c3ebfffb[m
Merge: d6db91a 414dc11
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jun 6 14:33:39 2018 -0700

    Merge changes from Christian's fork

[33mcommit d6db91a4c188aab3a7606274e2b2c467b8df21f4[m
Merge: 89564b6 db6ae13
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jun 6 13:21:01 2018 -0700

    Updating latest amp changes to use new C++ backend

[33mcommit 89564b69b421075e27ff564f5c13996775f29ca6[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jun 6 13:08:30 2018 -0700

    Update README

[33mcommit d506eff27cd860cf936b424c367dd5773a2d7ea2[m[33m ([m[1;31morigin/refactor_backend[m[33m)[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Jun 6 13:02:25 2018 -0700

    Macros based on torch.__version__ to compile with 0.4 and 0.5

[33mcommit 61b452e8941540d9ea935c3206ce5608b356539e[m
Merge: fb7d4e1 6143b30
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue Jun 5 16:18:10 2018 -0700

    Merging latest master

[33mcommit db6ae13a0ecb5225e66f732a2547437861dc6348[m
Author: Carl Case <carlc@nvidia.com>
Date:   Tue Jun 5 15:31:00 2018 -0700

    remove cuda check in build_cffi to handle docker installs

[33mcommit 6143b30fa8a4c0e95cd7567f8cf633bc826bb3e7[m
Merge: eb0e59e 38ea0e9
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Fri Jun 1 16:32:34 2018 -0700

    Merge pull request #7 from pooyadavoodi/world_size_multiproc
    
    Read world_size argument in multiproc if specified in argslist

[33mcommit eb0e59ebef8259e0fca13a6c2075f6a2d7f0205e[m
Merge: ea93767 dcda3b5
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Fri Jun 1 16:32:03 2018 -0700

    Merge pull request #8 from pooyadavoodi/speed_metric
    
    Print speed (img/sec) in imagenet example

[33mcommit dcda3b56b2ed0a01988d5827f8c7d705060c79ba[m
Author: Pooya Davoodi <pooyadavoodi@gmail.com>
Date:   Fri Jun 1 16:29:16 2018 -0700

    Print speed (img/sec) in imagenet example

[33mcommit 38ea0e906bbadd6225953664b5ee1ed04a9e5672[m
Author: Pooya Davoodi <pooyadavoodi@gmail.com>
Date:   Thu May 31 16:15:54 2018 -0700

    Read world_size argument in multiproc if specified in argslist

[33mcommit 5ae9fbc4498ee14983e44be7f5b43f8dbb0af5b5[m
Author: Carl Case <carlc@nvidia.com>
Date:   Wed May 30 14:43:41 2018 -0700

    readme

[33mcommit fb4190052421e8cc39d7d0816fd20905df639a1b[m
Author: Carl Case <carlc@nvidia.com>
Date:   Wed May 30 14:12:19 2018 -0700

    Hard ban on fp16 BCELoss

[33mcommit 6d30e1ffde3ec1b7b93e14432bb05eb39f0b58bc[m
Author: Carl Case <carlc@nvidia.com>
Date:   Wed May 30 13:37:34 2018 -0700

    rename build->init

[33mcommit ec431d334d074647e2ad20547603016200be47df[m
Author: Carl Case <carlc@nvidia.com>
Date:   Wed May 30 09:55:31 2018 -0700

    WIP: better annotation / user function registry support

[33mcommit 614b11ff4e08bd5828b05f54a35be90b3ec15e0b[m
Author: Carl Case <carlc@nvidia.com>
Date:   Tue May 29 16:39:01 2018 -0700

    support multi-loss scaling per-optimizer correctly

[33mcommit 8be1b0535329c706017f24e6b7ec2626c6c16e6d[m
Author: Carl Case <carlc@nvidia.com>
Date:   Tue May 29 16:38:10 2018 -0700

    bugfix: keep cache up-to-date on parameter require_grad-ness

[33mcommit 9cc744295df46e1f8e0c6f1b60a63a89cfae34d5[m
Author: Carl Case <carlc@nvidia.com>
Date:   Thu May 24 10:43:04 2018 -0700

    Optimizer wrapper; loss scaling class; no-op handle; start multi-loss

[33mcommit 414dc11970d81f85f847e4c2ffff99546fcaa248[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Sun May 27 00:35:54 2018 +0000

    Re-add normalization, correct typing.

[33mcommit fb7d4e1d6e5f882f2439c4eb1bad16cdc5f249b5[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri May 25 17:13:11 2018 -0700

    Fleshed out Cuda version checking and compiling for multiple arches

[33mcommit 0081afb8e780194be41630e421450c3ad326c95d[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Fri May 25 15:44:16 2018 -0700

    Decrease default message size.

[33mcommit cae6005c590b9ba1f3171cd51888a0f86bbbb93f[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Fri May 25 15:44:04 2018 -0700

    Update imagenet example to fast version.

[33mcommit d17a015f9bc3099e90e74a41ec1584a6fd76e14f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri May 25 15:15:18 2018 -0700

    Transferred backend and build system to use Pytorch C++ extension + ATen dispatch.

[33mcommit ea93767d22818bdd88ae738a8c7cf62b49a8fdaf[m
Author: Carl Case <carlc@nvidia.com>
Date:   Thu May 24 11:33:57 2018 -0700

    1804 RNN fix

[33mcommit 343590a1438af9a01dfb3e62eea916508017e01a[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Wed May 23 13:30:40 2018 -0700

    Revert "Revert distributed to simpler version until backward hooks are fixed."
    
    This reverts commit 47ac5c2bc03687b6ad0695be65edb4ce71cd13d1.

[33mcommit 1737ce1e9c7a25ad7c13f1f6428b6480c085673a[m
Merge: ee117aa 9ce3a33
Author: mcarilli <mcarilli@gmail.com>
Date:   Wed May 23 11:02:22 2018 -0700

    Merge pull request #4 from NVIDIA/amp_compat_fix
    
    Fix compatibility checks for 18.04 container

[33mcommit 9ce3a33d37de1de864b58a55d5104485146f6842[m
Author: Carl Case <carlc@nvidia.com>
Date:   Wed May 23 10:48:53 2018 -0700

    fix compatibility checks for 18.04 container

[33mcommit ee117aa8e0a672c2e0a5321931f3ac3a50f88a2c[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Wed May 23 10:19:21 2018 -0700

    Actually fix builds for pre 0.4

[33mcommit 4f8829f9baccc703f5aecf0f5dd1fa423656859e[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Wed May 23 09:56:26 2018 -0700

    Fix pre 0.4 builds.
    
    Fix regex expression to search for libaten

[33mcommit 70d2e3b744ab4300b79ba580ea25727928155b16[m
Merge: 2d5b71b 889d871
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Tue May 22 15:26:41 2018 -0700

    Merge pull request #3 from raulpuric/patch-2
    
    Create LARC.py

[33mcommit 889d871bbd84141adaa81fb1030695965769999a[m
Author: Raul Puri <raulpuric@berkeley.edu>
Date:   Tue May 22 14:43:31 2018 -0700

    Create LARC.py

[33mcommit 61c1e1602c76bac5809753c2e15f3cfce8efac19[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Mon May 21 14:58:11 2018 -0700

    Add device sync in imagenet example.

[33mcommit 2d5b71bde769c0599bc14433b0fce14865660861[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Mon May 21 14:58:11 2018 -0700

    Add device sync in imagenet example.

[33mcommit e733e78cf5c0ba5cc478fcbaf6eacc10fb9249a7[m
Author: Carl Case <carlc@nvidia.com>
Date:   Wed May 16 15:54:34 2018 -0700

    Initial support for automatic mixed precision

[33mcommit a3059288de4062cb98312c32d7f6f0827ba40e37[m
Merge: 8e1f01c 435133f
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Thu May 17 16:28:01 2018 -0700

    Merge pull request #1 from raulpuric/patch-1
    
    mLSTM cpu fix

[33mcommit 8e1f01c5190f4d1c0603ac1a3e9da257cb50bd40[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed May 16 01:36:17 2018 +0000

    Adding minimal examples with Apex and Pytorch distributed data parallel

[33mcommit 435133f1511b219b332c12fe17c73304171b7d3f[m
Author: Raul Puri <raulpuric@berkeley.edu>
Date:   Tue May 15 10:19:24 2018 -0700

    mLSTM cpu fix

[33mcommit 83acda92a800b7378d2e1b19c82d2ba9cae62d86[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Tue May 15 00:16:16 2018 +0000

    Fix build for pytorch post 0.4

[33mcommit cc8f03c85936e0261913ef32f1afb711f66160e4[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon May 14 23:50:35 2018 +0000

    Multi-op sequence for ddp_race_condition_test.py

[33mcommit 47ac5c2bc03687b6ad0695be65edb4ce71cd13d1[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Mon May 14 15:53:03 2018 -0700

    Revert distributed to simpler version until backward hooks are fixed.

[33mcommit d4066f6e629543bcd53ed8c528c70395a73cb9c3[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon May 14 22:38:33 2018 +0000

    Manually setting rank in imagenet examples

[33mcommit 9661dbd75c07fa3db0c28600283948565c12461f[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon May 14 18:41:09 2018 +0000

    Updating distributed example

[33mcommit 789afd89fe2c5a3e772f557055a9cf0f5e9d1241[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Wed May 9 21:02:33 2018 +0000

    Need to set_epoch in distributed example for sampler

[33mcommit 3ab105d9df607fb04b4446d19d485fbcb57cef40[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Tue May 8 17:38:43 2018 +0000

    Remove stub sentence in doc

[33mcommit 7c2ae41e7cf6feec983c6debb88190cc79ba7135[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Tue May 1 04:12:07 2018 +0000

    Fix race condition in DDP.

[33mcommit 531cb5c31070851edc431135fcaf8faad256681b[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon May 7 08:45:50 2018 -0700

    Updating word_language_model examples to evaluate with no_grad instead of volatile

[33mcommit 0d91a65e46c30e642d4cc28d702a5a60e8380d5d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Sat May 5 19:36:33 2018 -0700

    Adding main_fp16_optimizer.py to examples/imagenet and word_language_model

[33mcommit 9b76c2a2a79bd1d31045d4c75de651c12f5dfca8[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Fri May 4 11:51:31 2018 -0700

    Don't need libcuda for building.

[33mcommit 2c1cc0b5218ceea9890f2800db21744e9d157ddf[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Fri May 4 09:57:14 2018 -0700

    Typo

[33mcommit 8ef50822152653574a79e8cfb3651546a28b8f09[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu May 3 16:57:22 2018 -0700

    Adding examples that can serve as master copies for QA workflow moving forward.

[33mcommit 5dfa4c376829c757ac8dde0d8f11e2acfb69e956[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed May 2 10:40:52 2018 -0700

    Editing setup.py to locate nvcc and detect cuda major version more strictly

[33mcommit b0d7d60de5077b5b5179f14f458fe4d94f8e074d[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Tue May 1 18:58:14 2018 -0700

    Changing promotion of FP16_optimizer.optimizer attributes to use properties

[33mcommit 5709cfb57122ade7b8edfada1ff4ff834680f1a5[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Tue May 1 18:25:33 2018 +0000

    Try to improve robustness of finding cuda in build. Try to support building with CUDA 8.

[33mcommit 1cea1005734c233a60070baa3b138de024574247[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Tue May 1 17:42:13 2018 +0000

    Don't compile cuda code if running clean.

[33mcommit 3a08d827685761c58e4372bc6837cff0ab3f0db4[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Tue May 1 04:12:07 2018 +0000

    Distributed refactor.

[33mcommit dd05dbdbb982246da53ce9ff0278a0fadd957f6b[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Mon Apr 30 16:35:10 2018 -0700

    properly overriding __getstate__ and __setstate__

[33mcommit 7ce2e04fdfd4c71de7a1ee866bc025206597ef4c[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Fri Apr 27 17:04:43 2018 -0700

    Reorganizing fp16_optimizer

[33mcommit 31cee8e793e32237cf8762e0401528454c03ef80[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 26 15:03:04 2018 -0700

    Updating documentation

[33mcommit 262da9c628989031a01f4b350d7f8979cc285a84[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Thu Apr 26 10:20:17 2018 -0700

    Adding more helpful exception messages to prep_param_lists

[33mcommit f22201e7a20b4f66a18e1ed8d654cf3659313609[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Apr 25 16:49:36 2018 -0700

    step() now forwards return value

[33mcommit a3e2776a6495c65383aad761d5b61b25b56e7fd1[m
Author: Michael Carilli <mcarilli@nvidia.com>
Date:   Wed Apr 25 15:12:37 2018 -0700

    Cleaned comments in fp16_utils and csrc.  Keeping comments that are non-docstring but informative.

[33mcommit a288d585301ff78b07a1097d96e7ae862eadb683[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Wed Apr 25 17:54:34 2018 +0000

    FP16 util fix, detach at the right point.

[33mcommit 65167d21ba4fca955826061aaf020b35965e0318[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Wed Apr 25 10:39:29 2018 -0700

    Update README.md

[33mcommit 866454e0eb5bb8bb86982a47b5c1134d75ae4228[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Wed Apr 25 17:22:53 2018 +0000

    Distributed fix for non-all reduce call.

[33mcommit 2fa4dbafaae39a2eb1413c5d87e33252cd8e3b2c[m
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Wed Apr 25 16:46:16 2018 +0000

    Initial release
